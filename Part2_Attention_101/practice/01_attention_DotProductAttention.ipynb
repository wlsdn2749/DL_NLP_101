{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "    Attention 101 > Dot-Product Attention<br>\n", "        - this code is for educational purpose.<br>\n", "        - the code is written for easy understanding not for optimized code.<br>\n", "    Author : Sangkeun Jung (hugmanskj@gmai.com)<br>\n", "    All rights reserved. (2021)<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["In this code, we will implement<br>\n", "  - Dot-Product attention mechanism "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class DotAttention(nn.Module):\n", "    \"\"\"\n", "    Attention > Additive Attention > Bahdanau approach \n", "    Inputs:\n", "        query_vector  : [hidden_size]\n", "        multiple_items: [batch_size, num_of_items, hidden_size]\n", "    Returns:\n", "        blendded_vector:    [batch_size, item_vector hidden_size]\n", "        attention_scores:   [batch_size, num_of_items]\n", "    \"\"\"\n", "    def __init__(self, item_dim, query_dim, attention_dim):\n", "        super(DotAttention, self).__init__()\n", "        self.item_dim = item_dim            # dim. of multiple item vector\n", "        self.query_dim = query_dim          # dim. of query vector\n", "        self.attention_dim = attention_dim  # dim. of projected item or query vector\n", "        assert query_dim == item_dim, \"Dot attention require dim. of query and dim. of item is same.\"\n", "    def _calculate_reactivity(self, query_vector, multiple_items):\n", "        # 'dot' method try to get scalar value by dot operation\n", "        # see : [1,H] x [H,1] = [1,1] => [1] sclar value\n", "        query_vector = query_vector.unsqueeze(-1)  # [B,H] --> [B,H,1]\n\n", "        # [B,N,H] x [B,H,1] --> [B,N,1]\n", "        reactivity_scores = torch.bmm(multiple_items, query_vector) # [B, N, 1]\n", "        reactivity_scores = reactivity_scores.squeeze(-1) #[B,N]\n", "        return reactivity_scores\n", "    def forward(self, query_vector, multiple_items, mask):\n", "        \"\"\"\n", "        Inputs:\n", "            query_vector:   [query_vector hidden_size]\n", "            multiple_items: [batch_size, num_of_items, item_vector hidden_size]\n", "            mask:           [batch_size, num_of_items, num_of_items]  1 for valid item, 0 for invalid item\n", "        Returns:\n", "            blendded_vector:    [batch_size, item_vector hidden_size]\n", "            attention_scores:   [batch_size, num_of_items]\n", "        \"\"\"\n", "        assert mask is not None, \"mask is required\"\n\n", "        # B : batch_size, N : number of multiple items, H : hidden size of item\n", "        B, N, H = multiple_items.size() \n", "        \n", "        # Three Steps\n", "        # 1) [reactivity] try to check the reactivity with ( item_t and query_vector ) N times\n", "        # 2) [masking]    try to penalize invalid items such as <pad>\n", "        # 3) [attention]  try to get proper attention scores (=propability form) over the reactivity scores\n", "        # 4) [blend]      try to blend multiple items with attention scores\n\n", "        # Step-1) reactivity\n", "        reactivity_scores = self._calculate_reactivity(query_vector, multiple_items)\n\n", "        # Step-2) masking\n", "        # The mask marks valid positions so we invert it using `mask & 0`.\n", "        # detail : check the masked_fill_() of pytorch \n", "        reactivity_scores.data.masked_fill_(mask == 0, -float('inf'))\n\n", "        # Step-3) attention score\n", "        attention_scores = F.softmax(reactivity_scores, dim=1) # over the item dimensions\n\n", "        # Step-4) blend multiple items\n", "        # merge by weighted sum\n", "        attention_scores = attention_scores.unsqueeze(1) # [B, 1, #_of_items]\n\n", "        # [B, 1, #_of_items] * [B, #_of_items, dim_of_item] --> [B, 1, dim_of_item]\n", "        blendded_vector = torch.matmul(attention_scores, multiple_items) \n", "        blendded_vector = blendded_vector.squeeze(1) # [B, dim_of_item] \n", "        return blendded_vector, attention_scores"]}, {"cell_type": "markdown", "metadata": {}, "source": [" ------------------------------------------------------------------------ ##<br>\n", " Training and Testing with toy dataset                                    ##<br>\n", " ------------------------------------------------------------------------ ##"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pytorch_lightning as pl\n", "from torch.utils.data import Dataset, DataLoader\n", "import numpy as np "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def load_data(fn):\n", "    data = []\n", "    with open(fn, 'r', encoding='utf-8') as f:\n", "        for line in f:\n", "            line = line.rstrip()\n", "            seq_str, query, y = line.split('\\t')\n", "            seqs = seq_str.split(',')\n", "            data.append( (seqs, query, y) )\n", "    return data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["you can define any type of dataset<br>\n", "dataset : return an example for batch construction. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class NumberDataset(Dataset):\n", "    \"\"\"Dataset.\"\"\"\n", "    def __init__(self, fn, input_vocab, output_vocab, max_seq_length):\n", "        self.input_vocab = input_vocab\n", "        self.output_vocab = output_vocab\n", "        self.max_seq_length = max_seq_length \n", "        \n", "        # load \n", "        self.data = load_data(fn)\n", "    def __len__(self):\n", "        return len(self.data) \n", "    def __getitem__(self, idx): \n", "        seq, q, y = self.data[idx]\n\n", "        # [ input ]\n", "        seq_ids = [ self.input_vocab[t] for t in seq ]\n\n", "        # <pad> processing\n", "        pad_id      = self.input_vocab['<pad>']\n", "        num_to_fill = self.max_seq_length - len(seq)\n", "        seq_ids     = seq_ids + [pad_id]*num_to_fill\n\n", "        # mask processing (1 for valid, 0 for invalid)\n", "        weights = [1]*len(seq) + [0]*num_to_fill\n\n", "        # [ query ]\n", "        # NOTE : we assume that query vocab space is same as input vocab space\n", "        q_id = self.input_vocab[q] # enable valid query \n", "        #q_id = 0 # disable query -- to check query effect in attention mechanism\n\n", "        # [ ouput ] \n", "        y_id = self.output_vocab[y]\n", "        item = [\n", "                    # input\n", "                    np.array(seq_ids),\n", "                    q_id,\n", "                    np.array(weights),\n", "                    # output\n", "                    y_id\n", "               ]\n", "        return item "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class NumberDataModule(pl.LightningDataModule):\n", "    def __init__(self, \n", "                 max_seq_length: int=12,\n", "                 batch_size: int = 32):\n", "        super().__init__()\n", "        self.batch_size = batch_size\n", "        self.max_seq_length = max_seq_length \n", "        input_vocab, output_vocab = self.make_vocab('./data/numbers/train.txt')\n", "        self.input_vocab_size = len( input_vocab )\n", "        self.output_vocab_size = len( output_vocab )\n", "        self.padding_idx = input_vocab['<pad>']\n", "        self.input_r_vocab  = { v:k for k,v in input_vocab.items() }\n", "        self.output_r_vocab = { v:k for k,v in output_vocab.items() }\n", "        self.all_train_dataset = NumberDataset('./data/numbers/train.txt', input_vocab, output_vocab, max_seq_length)\n", "        self.test_dataset      = NumberDataset('./data/numbers/test.txt', input_vocab, output_vocab, max_seq_length)\n\n", "        # random split train / valiid for early stopping\n", "        N = len(self.all_train_dataset)\n", "        tr = int(N*0.8) # 8 for the training\n", "        va = N - tr     # 2 for the validation \n", "        self.train_dataset, self.valid_dataset = torch.utils.data.random_split(self.all_train_dataset, [tr, va])\n", "    def make_vocab(self, fn):\n", "        input_tokens = []\n", "        output_tokens = []\n", "        data = load_data(fn)\n", "        for seqs, query, y in data:\n", "            for token in seqs:\n", "                input_tokens.append(token)\n", "            output_tokens.append(y)\n", "        \n", "        input_tokens = list(set(input_tokens))\n", "        output_tokens = list(set(output_tokens)) \n", "        input_tokens.sort()\n", "        output_tokens.sort()\n\n", "        # [input vocab]\n", "        # add <pad> symbol to input tokens as a first item\n", "        input_tokens = ['<pad>'] + input_tokens \n", "        input_vocab = { str(token):index for index, token in enumerate(input_tokens) }\n\n", "        # [output voab]\n", "        output_vocab = { str(token):index for index, token in enumerate(output_tokens) }\n", "        return input_vocab, output_vocab\n", "    def train_dataloader(self):\n", "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True) # NOTE : Shuffle\n", "    def val_dataloader(self):\n", "        return DataLoader(self.valid_dataset, batch_size=self.batch_size)\n", "    def test_dataloader(self):\n", "        return DataLoader(self.test_dataset, batch_size=self.batch_size)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from torchmetrics import functional as FM"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class Attention_Number_Finder(pl.LightningModule): \n", "    def __init__(self, \n", "                 # network setting\n", "                 input_vocab_size,\n", "                 output_vocab_size,\n", "                 d_model,      # dim. in attemtion mechanism \n", "                 padding_idx,\n", "                 # optiimzer setting\n", "                 learning_rate=1e-3):\n", "        super().__init__()\n", "        self.save_hyperparameters()  "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["        # note \n", "        #   - the dimension for query and multi-items do not need to be same. \n", "        #   - for simplicity, we make all the dimensions as same. \n\n", "        # symbol_number_character to vector_number\n", "        self.digit_emb = nn.Embedding(self.hparams.input_vocab_size, \n", "                                      self.hparams.d_model, \n", "                                      padding_idx=self.hparams.padding_idx)\n\n", "        # sequence encoder using RNN\n", "        self.encoder = nn.LSTM(d_model, int(self.hparams.d_model/2), # why? since bidirectional LSTM\n", "                            num_layers=2, \n", "                            bidirectional=True,\n", "                            batch_first=True\n", "                          )\n\n", "        # attention mechanism\n", "        self.att = DotAttention(item_dim=self.hparams.d_model,\n", "                                query_dim=self.hparams.d_model,\n", "                                attention_dim=self.hparams.d_model)\n\n", "        # [to output]\n", "        self.to_output = nn.Linear(self.hparams.d_model, self.hparams.output_vocab_size) # D -> a single number\n\n", "        # loss\n", "        self.criterion = nn.CrossEntropyLoss()  \n", "    def forward(self, seq_ids, q_id, weight):\n", "        # ------------------- ENCODING with ATTENTION -----------------#\n", "        # [ Digit Character Embedding ]\n", "        # seq_ids : [B, max_seq_len]\n", "        seq_embs = self.digit_emb(seq_ids.long()) # [B, max_seq_len, emb_dim]\n\n", "        # [ Sequence of Numbers Encoding ]\n", "        seq_encs, _ = self.encoder(seq_embs) # [B, max_seq_len, enc_dim*2]  since we have 2 layers\n", "        \n", "        # with query (context)\n", "        query = self.digit_emb(q_id) # [B, query_dim]\n\n", "        # dynamic encoding-summarization (blending)\n", "        multiple_items = seq_encs"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["        blendded_vector, attention_scores = self.att(query, multiple_items, mask=weight) # [B, #_of_items]\n", "        # blendded_vector  : [B, dim_of_sequence_enc]\n", "        # attention_scores : [B, query_len, N] \n", "        blendded_vector = blendded_vector.squeeze(1) # since we use a single query \n", "        \n", "        # To output\n", "        logits = self.to_output(blendded_vector)\n", "        return logits, attention_scores\n", "    def training_step(self, batch, batch_idx):\n", "        seq_ids, q_id, weights, y_id = batch \n", "        logits, _ = self(seq_ids, q_id, weights)  # [B, output_vocab_size]\n", "        loss = self.criterion(logits, y_id.long()) \n", "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n\n", "        # all logs are automatically stored for tensorboard\n", "        return loss\n", "    def validation_step(self, batch, batch_idx):\n", "        seq_ids, q_id, weights, y_id = batch \n", "        logits, _ = self(seq_ids, q_id, weights)  # [B, output_vocab_size]\n", "        loss = self.criterion(logits, y_id.long()) \n", "        \n", "        ## get predicted result\n", "        prob = F.softmax(logits, dim=-1)\n", "        acc = FM.accuracy(prob, y_id)\n", "        metrics = {'val_acc': acc, 'val_loss': loss}\n", "        self.log_dict(metrics)\n", "        return metrics\n", "    def validation_step_end(self, val_step_outputs):\n", "        val_acc  = val_step_outputs['val_acc'].cpu()\n", "        val_loss = val_step_outputs['val_loss'].cpu()\n", "        self.log('validation_acc',  val_acc, prog_bar=True)\n", "        self.log('validation_loss', val_loss, prog_bar=True)\n", "    def test_step(self, batch, batch_idx):\n", "        seq_ids, q_id, weights, y_id = batch \n", "        logits, _ = self(seq_ids, q_id, weights)  # [B, output_vocab_size]\n", "        loss = self.criterion(logits, y_id.long()) \n", "        \n", "        ## get predicted result\n", "        prob = F.softmax(logits, dim=-1)\n", "        acc = FM.accuracy(prob, y_id)\n", "        metrics = {'test_acc': acc, 'test_loss': loss}\n", "        self.log_dict(metrics, on_epoch=True)\n", "        return metrics\n", "    def configure_optimizers(self):\n", "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n", "        return optimizer\n", "    @staticmethod\n", "    def add_model_specific_args(parent_parser):\n", "        parser = parent_parser.add_argument_group(\"ATTENTION\")\n", "        parser.add_argument('--learning_rate', type=float, default=0.00001)\n", "        return parent_parser"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "plt.rcParams['figure.figsize'] = [10, 8]\n", "def check_attention(model, ex, input_vocab, output_vocab):\n", "    seq_ids, q_id, weights, y_id = ex\n", "    seq_ids = seq_ids.to(model.device)\n", "    q_id = q_id.to(model.device)\n", "    weights = weights.to(model.device)\n", "    import os \n", "    os.makedirs('./output_figs/Dot', exist_ok=True)\n", "    import pandas as pd\n", "    # predictions\n", "    with torch.no_grad():\n", "        logits, att_scores = model(seq_ids, q_id, weights)  # [B, output_vocab_size]\n", "    \n", "        prob = F.softmax(logits, dim=-1)\n", "        y_id_pred = prob.argmax(dim=-1)\n", "        for idx, (a_seq_ids, a_q_id, a_weights, a_y_id, a_y_id_pred, a_att_scores) in enumerate( zip(seq_ids, q_id, weights, y_id, y_id_pred, att_scores) ):\n", "            N =  a_weights.sum().item()\n", "            input_sym = [ input_vocab[i.item()] for i in a_seq_ids[:N] ]\n", "            q_sym = input_vocab[a_q_id.item()]\n", "            ref_y_sym = output_vocab[a_y_id_pred.item()]\n", "            pred_y_sym = output_vocab[a_y_id_pred.item()]\n", "            scores = a_att_scores.cpu().detach().numpy()[0][:N].tolist() \n\n", "            ## heatmap\n", "            data = { 'scores':[] }\n", "            for word, score in zip(input_sym, scores):\n", "                data['scores'].append( score )\n", "                df = pd.DataFrame(data)\n", "            df.index = input_sym\n", "            plt.figure()\n", "            #sns.set(rc = {'figure.figsize':(2,8)})\n", "            sns.heatmap(df, cmap='RdYlGn_r')\n", "            plt.title(f'Finding the first larger value than query={q_sym}, ref={ref_y_sym}, pred={pred_y_sym}', fontsize=10)\n", "            plt.savefig(os.path.join('./output_figs/Dot', f'{idx}.png'))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from argparse import ArgumentParser\n", "from pytorch_lightning.callbacks import EarlyStopping\n", "def cli_main():\n", "    pl.seed_everything(1234)\n\n", "    # ------------\n", "    # args\n", "    # ------------\n", "    parser = ArgumentParser()\n", "    parser.add_argument('--batch_size', default=200, type=int)\n", "    parser.add_argument('--d_model',    default=512, type=int)  # dim. for attention model \n", "    parser = pl.Trainer.add_argparse_args(parser)\n", "    parser = Attention_Number_Finder.add_model_specific_args(parser)\n", "    args = parser.parse_args()\n\n", "    # ------------\n", "    # data\n", "    # ------------\n", "    dm = NumberDataModule.from_argparse_args(args)\n", "    iter(dm.train_dataloader()).next() # <for testing "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    # ------------\n", "    # model\n", "    # ------------\n", "    model = Attention_Number_Finder(dm.input_vocab_size,\n", "                                    dm.output_vocab_size,\n", "                                    args.d_model,       # dim. in attemtion mechanism \n", "                                    dm.padding_idx,\n", "                                    args.learning_rate)\n\n", "    # ------------\n", "    # training\n", "    # ------------\n", "    trainer = pl.Trainer(\n", "                            max_epochs=60, \n", "                            callbacks=[EarlyStopping(monitor='val_loss')],\n", "                            gpus = 1 # if you have gpu -- set number, otherwise zero\n", "                        )\n", "    trainer.fit(model, datamodule=dm)\n\n", "    # ------------\n", "    # testing\n", "    # ------------\n", "    result = trainer.test(model, test_dataloaders=dm.test_dataloader())\n", "    print(result)\n\n", "    #{'test_acc': 0.9039999842643738, 'test_loss': 0.2998247742652893}\n\n", "    # ------------\n", "    # Check the attention scores to attend on multiple items\n", "    # ------------\n", "    #model = Attention_Number_Finder.load_from_checkpoint('./lightning_logs/version_15/checkpoints/epoch=0-step=179.ckpt').to('cuda:0')\n", "    ex_batch = iter(dm.test_dataloader()).next()\n", "    check_attention(model, ex_batch, dm.input_r_vocab, dm.output_r_vocab)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == '__main__':\n", "    cli_main()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}