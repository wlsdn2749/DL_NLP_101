{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "    Transformer 101 > Encoder Part + MultiLayer Implementation<br>\n", "        - this code is for educational purpose.<br>\n", "        - the code is written for easy understanding not for optimized code.<br>\n", "    Author : Sangkeun Jung (hugmanskj@gmai.com)<br>\n", "    All rights reserved. (2021)<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["In this code, we will implement<br>\n", "  - Transformer Encoder Part with multiple layer<br>\n", "  - We wrap around pre-implemented TransformerEncoderLayer to make TransformerEncoder"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F"]}, {"cell_type": "markdown", "metadata": {}, "source": [" ------------------------------------------------------------------------ ##<br>\n", " Training and Testing with toy dataset                                    ##<br>\n", " ------------------------------------------------------------------------ ##"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pytorch_lightning as pl\n", "from torch.utils.data import Dataset, DataLoader\n", "import numpy as np "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def load_data(fn):\n", "    data = []\n", "    with open(fn, 'r', encoding='utf-8') as f:\n", "        for line in f:\n", "            line = line.rstrip()\n", "            query_item_seq_str, y = line.split('\\t')\n", "            all_tokens = query_item_seq_str.split(',')\n", "            q_tokens = all_tokens[0].split('|')\n", "            i_tokens = all_tokens[1:]\n", "            tokens = [q_tokens[0], '|'] + [q_tokens[1]] + i_tokens \n", "            data.append( (tokens, y) )\n", "    return data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["you can define any type of dataset<br>\n", "dataset : return an example for batch construction. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class NumberDataset(Dataset):\n", "    \"\"\"Dataset.\"\"\"\n", "    def __init__(self, fn, input_vocab, output_vocab, max_seq_length):\n", "        self.input_vocab = input_vocab\n", "        self.output_vocab = output_vocab\n", "        self.max_seq_length = max_seq_length \n", "        \n", "        # load \n", "        self.data = load_data(fn)\n", "    def __len__(self):\n", "        return len(self.data) \n", "    def __getitem__(self, idx): \n", "        seq, y = self.data[idx]\n\n", "        # [ input ]\n", "        seq_ids = [ self.input_vocab[t] for t in seq ]\n\n", "        # <pad> processing\n", "        pad_id      = self.input_vocab['<pad>']\n", "        num_to_fill = self.max_seq_length - len(seq)\n", "        seq_ids     = seq_ids + [pad_id]*num_to_fill\n\n", "        # mask processing (1 for valid, 0 for invalid)\n", "        weights = [1]*len(seq) + [0]*num_to_fill\n\n", "        # [ ouput ] \n", "        y_id = self.output_vocab[y]\n", "        item = [\n", "                    # input\n", "                    np.array(seq_ids),\n", "                    np.array(weights),\n", "                    # output\n", "                    y_id\n", "               ]\n", "        return item "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class NumberDataModule(pl.LightningDataModule):\n", "    def __init__(self, \n", "                 max_seq_length: int=15,\n", "                 batch_size: int = 32):\n", "        super().__init__()\n", "        self.batch_size = batch_size\n", "        self.max_seq_length = max_seq_length \n", "        input_vocab, output_vocab = self.make_vocab('./data/numbers/train.seq.txt')\n", "        self.input_vocab_size = len( input_vocab )\n", "        self.output_vocab_size = len( output_vocab )\n", "        self.padding_idx = input_vocab['<pad>']\n", "        self.input_r_vocab  = { v:k for k,v in input_vocab.items() }\n", "        self.output_r_vocab = { v:k for k,v in output_vocab.items() }\n", "        self.all_train_dataset = NumberDataset('./data/numbers/train.seq.txt', input_vocab, output_vocab, max_seq_length)\n", "        self.test_dataset      = NumberDataset('./data/numbers/test.seq.txt', input_vocab, output_vocab, max_seq_length)\n\n", "        # random split train / valiid for early stopping\n", "        N = len(self.all_train_dataset)\n", "        tr = int(N*0.8) # 8 for the training\n", "        va = N - tr     # 2 for the validation \n", "        self.train_dataset, self.valid_dataset = torch.utils.data.random_split(self.all_train_dataset, [tr, va])\n", "    def make_vocab(self, fn):\n", "        input_tokens = []\n", "        output_tokens = []\n", "        data = load_data(fn)\n", "        for tokens, y in data:\n", "            for token in tokens:\n", "                input_tokens.append(token)\n", "            output_tokens.append(y)\n", "        \n", "        input_tokens = list(set(input_tokens))\n", "        output_tokens = list(set(output_tokens)) \n", "        input_tokens.sort()\n", "        output_tokens.sort()\n\n", "        # [input vocab]\n", "        # add <pad> symbol to input tokens as a first item\n", "        input_tokens = ['<pad>'] + input_tokens \n", "        input_vocab = { str(token):index for index, token in enumerate(input_tokens) }\n\n", "        # [output voab]\n", "        output_vocab = { str(token):index for index, token in enumerate(output_tokens) }\n", "        return input_vocab, output_vocab\n", "    def train_dataloader(self):\n", "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True) # NOTE : Shuffle\n", "    def val_dataloader(self):\n", "        return DataLoader(self.valid_dataset, batch_size=self.batch_size)\n", "    def test_dataloader(self):\n", "        return DataLoader(self.test_dataset, batch_size=self.batch_size)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from torchmetrics import functional as FM"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from attentions import TransformerEncoderLayer"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import copy \n", "def clones(module, N):\n", "    \"Produce N identical layers.\"\n", "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class TransformerEncoder(nn.Module):\n", "    # Encoder Block - a stack of N layers\n", "    # Exactly same as TransformerEncoder \n", "    def __init__(self, num_layers, d_model, num_heads, dropout, dim_feedforward=None):\n", "        super(TransformerEncoder, self).__init__()\n", "        self.num_layers = num_layers\n", "        if dim_feedforward == None: dim_feedforward = 4*d_model  ## https://arxiv.org/pdf/1810.04805.pdf (page3)\n", "        \n", "        a_layer = TransformerEncoderLayer(d_model, num_heads, dim_feedforward, dropout)\n\n", "        # prepare N sub-blocks\n", "        self.layers = clones(a_layer, self.num_layers)\n", "        \n", "    def forward(self, x, mask=None):\n", "        # x expects : [B, seq_len, d_model] \n", "        layers_attn_scores = []\n", "        \"Pass the input (and mask) through each layer in turn.\"\n", "        for layer in self.layers:\n", "            x, attn_scores = layer(x, mask)\n", "            layers_attn_scores.append(attn_scores)\n", "        return x, layers_attn_scores"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["   \n", "class TransformerEncoder_Number_Finder(pl.LightningModule): \n", "    def __init__(self, \n", "                 # network setting\n", "                 input_vocab_size,\n", "                 output_vocab_size,\n", "                 d_model,      # dim. in attemtion mechanism \n", "                 num_heads,    # number of heads\n", "                 padding_idx,\n", "                 # optiimzer setting\n", "                 learning_rate=1e-3):\n", "        super().__init__()\n", "        self.save_hyperparameters()  \n\n", "        # symbol_number_character to vector_number\n", "        self.input_emb = nn.Embedding(self.hparams.input_vocab_size, \n", "                                      self.hparams.d_model, \n", "                                      padding_idx=self.hparams.padding_idx)\n\n", "        # Now, we use mult-layer transformer-encoder for encoding\n", "        #   - multiple items and a query item together\n", "        self.encoder = TransformerEncoder(num_layers=3,\n", "                                          d_model=self.hparams.d_model,\n", "                                          num_heads=self.hparams.num_heads,\n", "                                          dropout=0.1)\n", "        # to check print(self.encoder)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["        # [to output]\n", "        self.to_output = nn.Linear(self.hparams.d_model, self.hparams.output_vocab_size) # D -> a single number\n\n", "        # loss\n", "        self.criterion = nn.CrossEntropyLoss()  \n", "    def forward(self, seq_ids, weight):\n", "        # INPUT EMBEDDING\n", "        # [ Digit Character Embedding ]\n", "        # seq_ids : [B, max_seq_len]\n", "        seq_embs = self.input_emb(seq_ids.long()) # [B, max_seq_len, d_model]\n\n", "        # ENCODING BY Transformer-Encoder\n", "        # [mask shaping]\n", "        mask = weight[:, None, None, :] # [B, 1, 1, max_seq_len]\n", "        seq_encs, attention_scores = self.encoder(seq_embs, mask) # [B, max_seq_len, d_model] \n\n", "        # seq_encs : [B, max_seq_len, d_model]\n", "        # attention_scores : [B, max_seq_len_q, max_seq_len_k]\n\n", "        # Output Processing\n", "        blendded_vector = seq_encs[:,0]  # taking the first(query) - step hidden state\n", "        \n", "        # To output\n", "        logits = self.to_output(blendded_vector)\n", "        return logits, attention_scores\n", "    def training_step(self, batch, batch_idx):\n", "        seq_ids, weights, y_id = batch \n", "        logits, _ = self(seq_ids, weights)  # [B, output_vocab_size]\n", "        loss = self.criterion(logits, y_id.long()) \n", "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n\n", "        # all logs are automatically stored for tensorboard\n", "        return loss\n", "    def validation_step(self, batch, batch_idx):\n", "        seq_ids, weights, y_id = batch \n", "        logits, _ = self(seq_ids, weights)  # [B, output_vocab_size]\n", "        loss = self.criterion(logits, y_id.long()) \n", "        \n", "        ## get predicted result\n", "        prob = F.softmax(logits, dim=-1)\n", "        acc = FM.accuracy(prob, y_id)\n", "        metrics = {'val_acc': acc, 'val_loss': loss}\n", "        self.log_dict(metrics)\n", "        return metrics\n", "    def validation_step_end(self, val_step_outputs):\n", "        val_acc  = val_step_outputs['val_acc'].cpu()\n", "        val_loss = val_step_outputs['val_loss'].cpu()\n", "        self.log('validation_acc',  val_acc, prog_bar=True)\n", "        self.log('validation_loss', val_loss, prog_bar=True)\n", "    def test_step(self, batch, batch_idx):\n", "        seq_ids, weights, y_id = batch \n", "        logits, _ = self(seq_ids, weights)  # [B, output_vocab_size]\n", "        loss = self.criterion(logits, y_id.long()) \n", "        \n", "        ## get predicted result\n", "        prob = F.softmax(logits, dim=-1)\n", "        acc = FM.accuracy(prob, y_id)\n", "        metrics = {'test_acc': acc, 'test_loss': loss}\n", "        self.log_dict(metrics, on_epoch=True)\n", "        return metrics\n", "    def configure_optimizers(self):\n", "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n", "        return optimizer\n", "    @staticmethod\n", "    def add_model_specific_args(parent_parser):\n", "        parser = parent_parser.add_argument_group(\"ATTENTION\")\n", "        parser.add_argument('--learning_rate', type=float, default=0.0001)\n", "        return parent_parser"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from argparse import ArgumentParser\n", "from pytorch_lightning.callbacks import EarlyStopping\n", "def cli_main():\n", "    pl.seed_everything(1234)\n\n", "    # ------------\n", "    # args\n", "    # ------------\n", "    parser = ArgumentParser()\n", "    parser.add_argument('--batch_size', default=200, type=int)\n", "    parser.add_argument('--d_model',    default=512, type=int)  # dim. for attention model \n", "    parser.add_argument('--num_heads',  default=8, type=int)    # number of multi-heads\n", "    parser = pl.Trainer.add_argparse_args(parser)\n", "    parser = TransformerEncoder_Number_Finder.add_model_specific_args(parser)\n", "    args = parser.parse_args()\n\n", "    # ------------\n", "    # data\n", "    # ------------\n", "    dm = NumberDataModule.from_argparse_args(args)\n", "    iter(dm.train_dataloader()).next() # <for testing "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    # ------------\n", "    # model\n", "    # ------------\n", "    model = TransformerEncoder_Number_Finder(dm.input_vocab_size,\n", "                                    dm.output_vocab_size,\n", "                                    args.d_model,       # dim. in attemtion mechanism \n", "                                    args.num_heads,\n", "                                    dm.padding_idx,\n", "                                    args.learning_rate)\n\n", "    # ------------\n", "    # training\n", "    # ------------\n", "    trainer = pl.Trainer(\n", "                            max_epochs=2, \n", "                            callbacks=[EarlyStopping(monitor='val_loss')],\n", "                            gpus = 1 # if you have gpu -- set number, otherwise zero\n", "                        )\n", "    trainer.fit(model, datamodule=dm)\n\n", "    # ------------\n", "    # testing\n", "    # ------------\n", "    result = trainer.test(model, test_dataloaders=dm.test_dataloader())\n", "    print(result)\n\n", "    # {'test_acc': 1.0, 'test_loss': 0.00032059274963103235}\n", "    "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == '__main__':\n", "    cli_main()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}