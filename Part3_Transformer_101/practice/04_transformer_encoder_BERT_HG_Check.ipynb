{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "    Transformer 101 > BERT <br>\n", "        - this code is for educational purpose.<br>\n", "        - the code is written for easy understanding not for optimized code.<br>\n", "    Author : Sangkeun Jung (hugmanskj@gmai.com)<br>\n", "    All rights reserved. (2021)<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["In this code, we will implement<br>\n", "  - BERT (Bidirectional Encoder Representations from Transformer)<br>\n", "  - To implement, we re-use many parts of the pre-implemented TransformerEncoder<br>\n", "  - For better understanding, <br>\n", "      - check the paramter names of BERT original implementations and those of this implementation.<br>\n", "      - check how to copy huggingface parameter to this parameter"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from commons import TransformerEncoder"]}, {"cell_type": "markdown", "metadata": {}, "source": ["-------------------------------------------------------------------------- #<br>\n", "BERT Implementation<br>\n", "-------------------------------------------------------------------------- #<br>\n", "BERT has three parts<br>\n", "  - Embedding Part ( we will re-use huggingface code)<br>\n", "      - symbol embedding<br>\n", "      - position embedding<br>\n", "      - type embedding<br>\n", "<br>\n", "  - Transformer Encoder Blocks (we will user our own code)<br>\n", "<br>\n", "  - Pooling Part (we will re-use huggingface code)<br>\n", "<br>\n", "  Note that<br>\n", "      - embedding and pooling part varies a lot according to transformer researches.<br>\n", "-------------------------------------------------------------------------- #"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Embedding and Pooling<br>\n", "this embedding is from huggingface"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class BertEmbeddings(nn.Module):\n", "    \"\"\" this embedding moudles are from huggingface implementation\n", "        but, it is simplified for just testing \n", "    \"\"\"\n", "    def __init__(self, vocab_size, hidden_size, pad_token_id, max_bert_length_size, layer_norm_eps, hidden_dropout_prob):\n", "        super().__init__()\n", "        self.word_embeddings        = nn.Embedding(vocab_size, hidden_size, padding_idx=pad_token_id)\n", "        self.position_embeddings    = nn.Embedding(max_bert_length_size, hidden_size)\n", "        self.token_type_embeddings  = nn.Embedding(2, hidden_size) # why 2 ? 0 and 1 \n\n", "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n", "        # any TensorFlow checkpoint file\n", "        self.LayerNorm = nn.LayerNorm(hidden_size, eps=layer_norm_eps)\n", "        self.dropout   = nn.Dropout(hidden_dropout_prob)\n\n", "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n", "        self.register_buffer(\"position_ids\", torch.arange(max_bert_length_size).expand((1, -1)))\n", "        self.register_buffer(\n", "            \"token_type_ids\",\n", "            torch.zeros(self.position_ids.size(), dtype=torch.long, device=self.position_ids.device),\n", "            persistent=False,\n", "        )\n\n", "        # always absolute\n", "        self.position_embedding_type = \"absolute\"\n", "    def forward(self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0):\n", "        if input_ids is not None:\n", "            input_shape = input_ids.size()\n", "        else:\n", "            input_shape = inputs_embeds.size()[:-1]\n", "        seq_length = input_shape[1]\n", "        if position_ids is None:\n", "            position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n\n", "        # Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs\n", "        # when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves\n", "        # issue #5664\n", "        if token_type_ids is None:\n", "            if hasattr(self, \"token_type_ids\"):\n", "                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n", "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n", "                token_type_ids = buffered_token_type_ids_expanded\n", "            else:\n", "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n", "        if inputs_embeds is None:\n", "            inputs_embeds = self.word_embeddings(input_ids)\n", "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n", "        embeddings = inputs_embeds + token_type_embeddings\n", "        if self.position_embedding_type == \"absolute\":\n", "            position_embeddings = self.position_embeddings(position_ids)\n", "            embeddings += position_embeddings\n", "        embeddings = self.LayerNorm(embeddings)\n", "        embeddings = self.dropout(embeddings)\n", "        return embeddings"]}, {"cell_type": "markdown", "metadata": {}, "source": ["this pooler is from huggingface"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class BertPooler(nn.Module):\n", "    def __init__(self, hidden_size):\n", "        super().__init__()\n", "        self.dense = nn.Linear(hidden_size, hidden_size)\n", "        self.activation = nn.Tanh()\n", "    def forward(self, hidden_states):\n", "        # We \"pool\" the model by simply taking the hidden state corresponding\n", "        # to the first token.\n", "        first_token_tensor = hidden_states[:, 0]\n", "        pooled_output = self.dense(first_token_tensor)\n", "        pooled_output = self.activation(pooled_output)\n", "        return pooled_output"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def cp_weight(src, tar, copy_bias=True, include_eps=False):\n", "    assert tar.weight.size() == src.weight.size(), \"Not compatible parameter size\"\n", "    tar.load_state_dict( src.state_dict() )\n", "    \n", "    if include_eps:\n", "        # in case of LayerNorm. \n", "        with torch.no_grad():\n", "            tar.eps = src.eps  \n\n", "    ## call by reference\n", "    ## therefore, tar value is changed in this func. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def cp_encoder_block_weights_from_huggingface(src_encoder, tar_encoder):\n", "    ## src: huggingface BERT model\n", "    ## tar: my BERT model \n", "    for layer_num, src_layer in enumerate(src_encoder.layer):\n", "        # <<< to MultiHeadAttention (wq, wk, wv, wo) >>>\n", "        cp_weight(src_layer.attention.self.query,   tar_encoder.layers[layer_num].self_attn.wq) # wq\n", "        cp_weight(src_layer.attention.self.key,     tar_encoder.layers[layer_num].self_attn.wk) # wk\n", "        cp_weight(src_layer.attention.self.value,   tar_encoder.layers[layer_num].self_attn.wv) # wv\n", "        cp_weight(src_layer.attention.output.dense, tar_encoder.layers[layer_num].self_attn.wo) # wo\n\n", "        # <<< to MLP (fc1, fc2) >>>\n", "        cp_weight(src_layer.intermediate.dense, tar_encoder.layers[layer_num].fc1) # feed_forward_1\n", "        cp_weight(src_layer.output.dense,       tar_encoder.layers[layer_num].fc2) # feed_forward_2\n\n", "        # layer normalization parameters\n", "        cp_weight(src_layer.attention.output.LayerNorm, tar_encoder.layers[layer_num].self_attn_layer_norm, include_eps=True) # norm_1\n", "        cp_weight(src_layer.output.LayerNorm,           tar_encoder.layers[layer_num].final_layer_norm, include_eps=True) # norm_2\n", "    return tar_encoder"]}, {"cell_type": "markdown", "metadata": {}, "source": [" Our Implemenation "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from argparse import ArgumentParser\n", "from pytorch_lightning.callbacks import EarlyStopping\n", "def cli_main():\n", "    ## prepare huggingface BERT\n", "    ##  - huggingface transformer is directly copyied from tensorflow's pretrained models\n\n", "    ## In the below, HG stands for 'huggingface'\n", "    from transformers import BertModel, BertTokenizer, BertConfig\n", "    model_name = 'bert-base-cased'\n", "    tokenizer  = BertTokenizer.from_pretrained(model_name)\n", "    hg_bert    = BertModel.from_pretrained(model_name) ## huggingface bert\n", "    hg_config  = BertConfig.from_pretrained(model_name)\n\n", "    ## prepare my BERT \n", "    ## \n", "    ## We need to prepare and copy networks and weights \n", "    ##      - Embeddings\n", "    ##      - Encoder\n", "    ##      - Pooler\n", "    # in case of bert-base-cased\n", "    input_vocab_size    = tokenizer.vocab_size \n", "    padding_idx         = tokenizer.convert_tokens_to_ids('[PAD]')\n", "    BERT_MAX_SEQ_LENGTH = hg_config.max_position_embeddings\n", "    d_model             = hg_config.hidden_size\n\n", "    # 1) BertBembeddings class code is from huggingface\n", "    embeddings = BertEmbeddings(\n", "                                        input_vocab_size,\n", "                                        d_model,\n", "                                        padding_idx,\n", "                                        BERT_MAX_SEQ_LENGTH,\n", "                                        hg_config.layer_norm_eps,     # layer norm eps\n", "                                        hg_config.hidden_dropout_prob # 0.1    \n", "                                )\n", "    embeddings.load_state_dict( hg_bert.embeddings.state_dict() ) # copy parameters"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    # 2) BertPooler class code is from huggingface\n", "    pooler = BertPooler(d_model)\n", "    pooler.load_state_dict( hg_bert.pooler.state_dict() ) # copy parameters\n\n", "    # 3) Encoder Block Prepare and Parameter Copying\n", "    encoder = TransformerEncoder(\n", "                                        num_layers=hg_config.num_hidden_layers, \n", "                                        d_model=d_model, \n", "                                        num_heads=hg_config.num_attention_heads,\n", "                                        dropout=hg_config.attention_probs_dropout_prob,\n", "                                        dim_feedforward=hg_config.intermediate_size\n", "                                )\n", "    encoder = cp_encoder_block_weights_from_huggingface(\n", "                                        src_encoder=hg_bert.encoder,\n", "                                        tar_encoder=encoder \n", "                                      )\n", "    input_texts =   [\n", "                        \"this is a test text\", \n", "                        \"is it working?\"\n", "                    ]\n", "                \n", "    tokenized_ouptut = tokenizer(input_texts, max_length=BERT_MAX_SEQ_LENGTH, padding=\"max_length\")\n", "    input_ids        = torch.tensor(tokenized_ouptut.input_ids)\n", "    o_attention_mask = torch.tensor(tokenized_ouptut.attention_mask)\n", "    token_type_ids   = torch.tensor(tokenized_ouptut.token_type_ids)\n", "    with torch.no_grad():\n", "        ## disable dropout -- huggingface\n", "        hg_bert.eval() \n\n", "        ## disable dropout -- my code\n", "        embeddings.eval() \n", "        pooler.eval() \n", "        encoder.eval() \n\n", "        ## now we need to feedforward both on huggingface BERT and our BERT\n", "        attention_mask = o_attention_mask[:, None, None, :] # [B, 1, 1, seq_len] \n", "        seq_embs   = embeddings(input_ids) \n", "        output     = encoder(seq_embs, attention_mask)\n", "        pooled_out = pooler(output[0]) \n", "        hg_output = hg_bert( \n", "                            input_ids=input_ids,\n", "                            attention_mask=o_attention_mask,\n", "                            token_type_ids=token_type_ids\n", "                          )\n", "        assert torch.all( torch.eq(hg_output.pooler_output, pooled_out) ), \"Not same result!\"\n", "        print(\"\\n\\nSAME RESULT! -- Huggingface and My Code\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == '__main__':\n", "    cli_main()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}