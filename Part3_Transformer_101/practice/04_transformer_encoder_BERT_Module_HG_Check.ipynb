{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "    Transformer 101 > BERT > more Modulized version<br>\n", "        - this code is for educational purpose.<br>\n", "        - the code is written for easy understanding not for optimized code.<br>\n", "    Author : Sangkeun Jung (hugmanskj@gmai.com)<br>\n", "    All rights reserved. (2021)<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In this code, we will implement<br>\n", "  - BERT (Bidirectional Encoder Representations from Transformer)<br>\n", "  - To implement, we re-use many parts of the pre-implemented TransformerEncoder<br>\n", "  - For better understanding, <br>\n", "      - check the paramter names of BERT original implementations and those of this implementation.<br>\n", "      - check how to copy huggingface parameter to this parameter<br>\n", "  - All the previous things are wrapped in 'BERT' in commons.py"]}, {"cell_type": "markdown", "metadata": {}, "source": [" Our Implemenation "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from argparse import ArgumentParser\n", "from pytorch_lightning.callbacks import EarlyStopping\n", "def cli_main():\n", "    ## prepare huggingface BERT\n", "    ##  - huggingface transformer is directly copyied from tensorflow's pretrained models\n\n", "    ## In the below, HG stands for 'huggingface'\n", "    from transformers import BertModel, BertTokenizer, BertConfig\n", "    model_name = 'bert-base-cased'\n", "    tokenizer  = BertTokenizer.from_pretrained(model_name)\n", "    hg_bert    = BertModel.from_pretrained(model_name) ## huggingface bert\n", "    hg_config  = BertConfig.from_pretrained(model_name)\n", "    from commons import BERT_CONFIG\n", "    my_config = BERT_CONFIG(\n", "                            vocab_size=tokenizer.vocab_size,\n", "                            padding_idx=tokenizer.convert_tokens_to_ids('[PAD]'),\n", "                            max_seq_length=hg_config.max_position_embeddings,\n", "                            d_model=hg_config.hidden_size,\n", "                            layer_norm_eps=hg_config.layer_norm_eps,\n", "                            emb_hidden_dropout=hg_config.hidden_dropout_prob,\n", "                            num_layers=hg_config.num_hidden_layers,\n", "                            num_heads=hg_config.num_attention_heads,\n", "                            att_prob_dropout=hg_config.attention_probs_dropout_prob,\n", "                            dim_feedforward=hg_config.intermediate_size\n", "                           )    \n", "    from commons import BERT\n", "    my_bert = BERT(my_config)\n", "    my_bert.copy_weights_from_huggingface(hg_bert)\n", "    \n", "    input_texts =   [\n", "                        \"this is a test text\", \n", "                        \"is it working?\"\n", "                    ]\n", "                \n", "    tokenized_ouptut = tokenizer(input_texts, max_length=my_config.max_seq_length, padding=\"max_length\")\n", "    input_ids        = torch.tensor(tokenized_ouptut.input_ids)\n", "    o_attention_mask = torch.tensor(tokenized_ouptut.attention_mask)\n", "    token_type_ids   = torch.tensor(tokenized_ouptut.token_type_ids)\n", "    with torch.no_grad():\n", "        ## disable dropout -- huggingface\n", "        hg_bert.eval() \n", "        hg_output = hg_bert( \n", "                            input_ids=input_ids,\n", "                            attention_mask=o_attention_mask,\n", "                            token_type_ids=token_type_ids\n", "                          )\n\n", "        ## disable dropout -- my code\n", "        my_bert.eval()\n", "        my_output, my_layer_att_scores = my_bert(input_ids=input_ids, \n", "                                                 token_type_ids=token_type_ids,\n", "                                                 attention_mask=o_attention_mask)\n", "        assert torch.all( torch.eq(hg_output.pooler_output, my_output) ), \"Not same result!\"\n", "        print(\"\\n\\nSAME RESULT! -- Huggingface and My Code\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == '__main__':\n", "    cli_main()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}