{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "    Transformer 101 > Decoder only architecture (GPT2)<br>\n", "        - this code is for educational purpose.<br>\n", "        - the code is written for easy understanding not for optimized code.<br>\n", "    Author : Sangkeun Jung (hugmanskj@gmai.com)<br>\n", "    All rights reserved. (2021)<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["In this code, we will implement<br>\n", "  - GPT2 architecture<br>\n", "      - Decoder (we will implement it from the scratch)<br>\n", "  - Check carefully, How to implement<br>\n", "      - remove Cross-Attention<br>\n", "      - Post-LN to Pre-LN architecture !!!!<br>\n", "  - Check huggingface GPT2 and our implementation shows the exact results. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import copy \n", "import math"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from commons import clones"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class Conv1D(nn.Module):\n", "    # this code is from https://amaarora.github.io/2020/02/18/annotatedGPT2.html -- and huggingface code\n", "    # basically, it is quite ok to use Linear instead of Conv1D\n", "    # but, to keep the consistency of original openai-gpt2 implmentation\n", "    # we used conv1d as well.\n", "    def __init__(self, nx, nf):\n", "        super().__init__()\n", "        self.nf = nf\n", "        w = torch.empty(nx, nf)\n", "        nn.init.normal_(w, std=0.02)\n", "        self.weight = nn.Parameter(w)\n", "        self.bias   = nn.Parameter(torch.zeros(nf))\n", "    def forward(self, x):\n", "        # [B, S, dim_x] -> [B, S, dim_nf]\n", "        size_out = x.size()[:-1] + (self.nf,)\n", "        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n", "        x = x.view(*size_out)\n", "        return x"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def gelu_new(x):\n", "    \"\"\"\n", "    this code is from https://github.com/huggingface/transformers/blob/3fefa292c1c419f0c4c3e2697cdd94cafaeb4b66/src/transformers/activations.py#L37\n", "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT). Also see\n", "    the Gaussian Error Linear Units paper: https://arxiv.org/abs/1606.08415\n", "    \"\"\"\n", "    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class GPT2MLP(nn.Module):\n", "    # this code is from https://amaarora.github.io/2020/02/18/annotatedGPT2.html\n", "    # renamed feedforward as MLP to follow openai-gpt2 implementation\n", "    # compare below GPT2MLP_linear_version\n", "    def __init__(self, d_model, nx, dropout):\n", "        super().__init__()\n", "        self.c_fc    = Conv1D(d_model, nx) # linear 1\n", "        self.c_proj  = Conv1D(nx, d_model) # linear 2\n", "        #self.act     = F.gelu\n", "        self.act     = gelu_new  # <-- to get exact same result of huggingface. you should use it\n", "        self.dropout = nn.Dropout(dropout)\n", "        \n", "    def forward(self, x):\n", "        return self.dropout(self.c_proj(self.act(self.c_fc(x))))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class GPT2MLP_linear_version(nn.Module):\n", "    # CONV1D equivalent Linear implementation\n", "    # but, you cannot import huggingface weights to this module.\n", "    def __init__(self, d_model, dim_feedforward=2048, dropout=0.1):\n", "        super(GPT2MLP, self).__init__()\n", "        self.feedforward_1 = nn.Linear(d_model, dim_feedforward)\n", "        self.act_function  = nn.GELU()\n", "        self.feedforward_2 = nn.Linear(dim_feedforward, d_model)\n", "        self.dropout = nn.Dropout(dropout)\n", "    def forward(self, x):\n", "        x = self.feedforward_1(x)\n", "        x = self.act_function(x)\n", "        x = self.feedforward_2(x)\n", "        x = self.dropout(x)\n", "        return x"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class MultiHeadAttention(nn.Module):\n", "    # from https://amaarora.github.io/2020/02/18/annotatedGPT2.html\n", "    # but, attention mask is not fully supported. so, I modified it.\n", "    def __init__(self, d_model, n_head, bias=True):\n", "        super().__init__()\n", "        self.n_head  = n_head\n", "        self.d_model = d_model\n", "        self.c_attn  = Conv1D(d_model, d_model*3)  # wegiht multiply part is replaced with Conv1D\n", "        self.dropout = nn.Dropout(0.1)\n", "        self.c_proj  = Conv1D(d_model, d_model)    # wegiht multiply part is replaced with Conv1D\n", "        \n", "        # We assume d_v always equals d_k\n", "        assert d_model % n_head == 0\n", "        self.d_k = d_model // self.n_head  # ex) d_model = 512, num_head = 8 --> d_k = 64\n", "    def split_heads(self, x):\n", "        new_shape = x.size()[:-1] + (self.n_head, self.d_k) \n", "        x = x.view(*new_shape)\n", "        return x.permute(0, 2, 1, 3) #[B, heads, seq_len, d_k]\n", "    def _attn(self, q, k, v, mask=None):\n", "        scores  = torch.matmul(q, k.transpose(-2, -1))\n", "        scores  = scores/math.sqrt(v.size(-1))  # scaling by root\n", "        nd, ns  = scores.size(-2), scores.size(-1)\n\n", "        # masking \n", "        if mask != None:\n", "            # sum-method\n", "            # https://github.com/huggingface/transformers/blob/3fefa292c1c419f0c4c3e2697cdd94cafaeb4b66/src/transformers/models/gpt2/modeling_gpt2.py#L809\n", "            mask = (1.0 - mask) * -1e4   ## follow hugging-face method  \n", "            scores = scores + mask   \n", "        scores  = F.softmax(scores, dim=-1) \n", "        scores  = self.dropout(scores)\n", "        outputs = torch.matmul(scores, v)\n", "        return outputs, scores\n", "    \n", "    def merge_heads(self, x):\n", "        x         = x.permute(0, 2, 1, 3).contiguous()\n", "        new_shape = x.size()[:-2] + (x.size(-2)*x.size(-1),)\n", "        return x.view(*new_shape)\n", "        \n", "    def forward(self, x, attention_mask):\n", "        x        = self.c_attn(x) # new `x` shape - `[1,3,2304]`\n", "        q, k, v  = x.split(self.d_model, dim=2)\n", "        q, k, v  = self.split_heads(q), self.split_heads(k), self.split_heads(v)\n", "        out, scores = self._attn(q, k, v, attention_mask)\n", "        out      = self.merge_heads(out)\n", "        out      = self.c_proj(out)\n", "        return out, scores"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class GPT2_TransformerBlock(nn.Module):\n", "    def __init__(self, d_model, n_head, dim_feedforward, dropout=0.1):\n", "        super(GPT2_TransformerBlock, self).__init__()\n", "        self.attn        = MultiHeadAttention(d_model=d_model, n_head=n_head, bias=True)\n", "        self.mlp         = GPT2MLP(d_model=d_model, nx=dim_feedforward, dropout=dropout)\n", "        self.ln_1        = nn.LayerNorm(d_model)\n", "        self.ln_2        = nn.LayerNorm(d_model)\n", "                \n", "    def forward(self, x, look_ahead_mask):\n", "        # Note : PRE Layer Normalization\n", "        # Note : attention mask for GPT2 block is only look-ahead-mask\n", "        # 1) layernorm and masked multihead \n", "        nx = self.ln_1(x)\n", "        a, attn_scores = self.attn(nx, attention_mask=look_ahead_mask) \n", "        x = x + a \n\n", "        # 2) layernorm and MLP\n", "        m = self.mlp( self.ln_2(x) )\n", "        x = x + m \n", "        return x, attn_scores"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class GPT2Decoder(nn.Module):\n", "    \"Decoder Block of GPT2 - a stack of N layers\"\n", "    #   - the position of LayerNorm is different from original implementation\n", "    #   - no encoder connected parts\n", "    def __init__(self, num_layers, d_model, num_heads, dim_feedforward=None):\n", "        super(GPT2Decoder, self).__init__()\n", "        self.num_layers = num_layers\n", "        if dim_feedforward == None: dim_feedforward = 4*d_model  ## https://arxiv.org/pdf/1810.04805.pdf (page3)\n", "        \n", "        a_layer = GPT2_TransformerBlock(d_model=d_model, n_head=num_heads, dim_feedforward=dim_feedforward)\n\n", "        # prepare N sub-blocks\n", "        self.layers = clones(a_layer, self.num_layers)\n", "        \n", "    def forward(self, x, look_ahead_mask=None):\n", "        # x : [B, tar_seq_len, d_model] \n", "        # enc_output : [B, src_seq_len, d_model] \n", "        # look_ahead_mask : \n", "        layers_attn_scores = []\n", "        \"Pass the input (and mask) through each layer in turn.\"\n", "        for layer in self.layers:\n", "            x, attn_scores = layer(x, look_ahead_mask)\n", "            layers_attn_scores.append(attn_scores)\n", "        return x, layers_attn_scores"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class GPT2(nn.Module):\n", "    \"\"\" GPT2 model \"\"\"\n", "    def __init__(self, \n", "                 vocab_size,    # decoder use same vocab for input and output\n", "                 num_layers,    # number of layers\n", "                 emb_dim,       # number embedding\n", "                 d_model,       # dim. in attemtion mechanism \n", "                 num_heads,\n", "                 max_seq_length,\n", "                 ):\n", "        super().__init__()\n", "        self.max_seq_len = max_seq_length\n", "        self.dropout_rate = 0.1 \n", "        self.dim_feedforward = 4 * d_model  # to follow convention (transformer)\n", "        self.tokens = 0\n\n", "        # GPT INPUT PART ---------------------------------\n", "        self.wte = nn.Embedding(vocab_size, emb_dim)       # input vocab size -> emb_dim\n", "        self.wpe = nn.Embedding(self.max_seq_len, emb_dim) # each position -> emb_dim\n", "        self.emb_dropout = nn.Dropout(self.dropout_rate)\n", "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n", "        self.register_buffer(\"position_ids\", torch.arange(self.max_seq_len).expand((1, -1)))\n\n", "        # GPT TRANSFORMER PART ---------------------------\n", "        self.blocks = GPT2Decoder(\n", "                                        num_layers=num_layers,\n", "                                        d_model=d_model,\n", "                                        num_heads=num_heads,\n", "                                        dim_feedforward=self.dim_feedforward\n", "                                 )\n", "        self.ln_f   = nn.LayerNorm(d_model) # to follow original gpt2 variable name\n\n", "        # GPT OUTPUT PART --------------------------------\n", "        # highgly depend on the task \n", "        # decoder head\n", "        self.head = nn.Linear(emb_dim, vocab_size, bias=False)\n", "    def forward(self, input_ids):\n", "        B, seq_len = input_ids.size()\n", "        assert seq_len <= self.max_seq_len, \"Input sequence length exceed model's maximum input length\"\n", "        \n", "        # ---- INPUT (EMBEDDING)  PART -----\n", "        token_embeddings = self.wte(input_ids) # each index maps to a (learnable) vector\n", "        seq_length = input_ids.shape[1]\n", "        position_ids = self.position_ids[:, :seq_length]\n", "        position_embeddings = self.wpe(position_ids) # each position maps to a (learnable) vector\n", "        x = self.emb_dropout(token_embeddings + position_embeddings)\n", "        \n", "        # ---- Transformer PART ------------\n", "        lookahead_mask = self.look_ahead_mask(seq_len).to(x.device) # mask : head compatible form.\n", "        x, layer_attn_scores = self.blocks(x, look_ahead_mask=lookahead_mask)\n", "        x = self.ln_f(x)  # <-- layer norm on the final transformer block\n\n", "        # --- OUTPUT PART ------------------\n", "        logits = self.head(x)\n", "        return logits\n", "    def look_ahead_mask(self, tgt_len:int) -> torch.FloatTensor:  \n", "        mask = torch.triu(torch.ones(tgt_len, tgt_len, dtype=torch.int), diagonal=1)\n", "        mask = 1 - mask # reverse\n", "        return mask\n", "        \n", "    "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def cp_weight(src, tar, copy_bias=True, include_eps=False):\n", "    assert tar.weight.size() == src.weight.size(), \"Not compatible parameter size\"\n", "    tar.load_state_dict( src.state_dict() )\n", "    \n", "    if include_eps:\n", "        # in case of LayerNorm. \n", "        with torch.no_grad():\n", "            tar.eps = src.eps  \n\n", "    ## call by reference\n", "    ## therefore, tar value is changed in this func. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def cp_gpt2_transformer_block_weights(src, tar):\n", "    ## src: huggingface GPT2 - Transformer model \n", "    ## tar: my GPT2 - model - core weights\n\n", "    ## layer normalization at top transformer block \n", "    cp_weight(src.transformer.ln_f, tar.ln_f, include_eps=True) # ln_f\n\n", "    ## layer weights\n", "    for layer_num, src_block in enumerate(src.transformer.h):\n", "        # <<< MultiHeadAttention (Conv1D's parameters) >>>\n", "        cp_weight(src_block.attn.c_attn,        tar.blocks.layers[layer_num].attn.c_attn) # c_attn\n", "        cp_weight(src_block.attn.c_proj,        tar.blocks.layers[layer_num].attn.c_proj) # c_proj\n\n", "        # same dropout for attention, residual, and others\n", "        #tar.blocks.layers[layer_num].attn.dropout.load_state_dict( src_block.attn.attn_dropout )\n\n", "        # <<< MLP >>\n", "        cp_weight(src_block.mlp.c_fc,       tar.blocks.layers[layer_num].mlp.c_fc) # c_fc\n", "        cp_weight(src_block.mlp.c_proj,     tar.blocks.layers[layer_num].mlp.c_proj) # c_proj\n", "        #tar.blocks.layers[layer_num].mlp.dropout.load_state_dict(src_block.mlp.dropout) # dropout\n\n", "        # layer normalization parameters\n", "        cp_weight(src_block.ln_1, tar.blocks.layers[layer_num].ln_1, include_eps=True) # ln_1\n", "        cp_weight(src_block.ln_2, tar.blocks.layers[layer_num].ln_2, include_eps=True) # ln_2\n", "    return tar"]}, {"cell_type": "markdown", "metadata": {}, "source": [" Our Implemenation "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from argparse import ArgumentParser\n", "from pytorch_lightning.callbacks import EarlyStopping\n", "def cli_main():\n", "    # -------------- GPT2 model -------------- ##\n", "    from transformers import GPT2Model, GPT2LMHeadModel\n", "    from transformers import GPT2Tokenizer\n", "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n", "    inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n\n", "    ## huggingface model\n", "    hg_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    ## my model\n", "    my_model = GPT2(\n", "                        vocab_size=hg_model.config.vocab_size, \n", "                        num_layers=hg_model.config.n_layer,\n", "                        emb_dim=hg_model.config.n_embd,\n", "                        d_model=hg_model.config.n_embd,\n", "                        num_heads=hg_model.config.n_head,\n", "                        max_seq_length=hg_model.config.n_ctx,\n", "                    )\n\n", "    ## [INPUT EMBEDDING] \n", "    ## copy embeddings from huggingface to my gpt2\n", "    my_model.wte.load_state_dict( hg_model.transformer.wte.state_dict() )\n", "    my_model.wpe.load_state_dict( hg_model.transformer.wpe.state_dict() )\n\n", "    ## [OUTPUT EMBEDDING]\n", "    ## copy to output vocab\n", "    my_model.head.load_state_dict( hg_model.lm_head.state_dict() )\n\n", "    ## [TRANSFORMER BLOCK]\n", "    ## transformer block copy \n", "    my_model = cp_gpt2_transformer_block_weights(hg_model, my_model)\n", "    hg_model.eval()\n", "    my_model.eval()\n", "    with torch.no_grad():\n", "        hg_outputs = hg_model(\n", "                            input_ids=inputs.input_ids,\n", "                            attention_mask=inputs.attention_mask\n", "                        )\n", "        my_output = my_model(\n", "                            input_ids=inputs.input_ids,\n", "                            #attention_mask=inputs.attention_mask  # <-- we don't need padding mask for GPT1, GPT2\n", "                        )\n", "        assert torch.all( torch.eq(hg_outputs.logits, my_output) ), \"Not same result!\"\n", "        print(\"SAME RESULT! -- Huggingface-GPT2 and My Code\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == '__main__':\n", "    cli_main()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}