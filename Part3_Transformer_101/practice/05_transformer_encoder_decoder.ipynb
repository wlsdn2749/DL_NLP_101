{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "    Transformer 101 > Encoder + Decoder<br>\n", "        - this code is for educational purpose.<br>\n", "        - the code is written for easy understanding not for optimized code.<br>\n", "    Author : Sangkeun Jung (hugmanskj@gmai.com)<br>\n", "    All rights reserved. (2021)<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["In this code, we will implement<br>\n", "  - Original Transformer <br>\n", "      - Encoder (we will re-use encoder implementations)<br>\n", "      - Decoder (we will implement it from the scratch)<br>\n", "  - Check carefully, How to implement<br>\n", "      - Cross-Attention (for giving encoder info. to decoder)<br>\n", "      - Look-ahead Masking (for ignoring future-information)<br>\n", "  - Also note that<br>\n", "      - encoder sequence length might not same as decoder sequence length<br>\n", "      - carefully, check query length and key length<br>\n", "<br>\n", "  - For the test dataset, <br>\n", "      - We will use number sorting dataset <br>\n", "      - Generate ordered sequences of numbers removing duplicated numbers"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import math"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from transformers.models.bert.tokenization_bert import VOCAB_FILES_NAMES "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def result_collapse(outputs, target):\n", "    if len( [x[target] for x in outputs][0].shape ) == 0:\n", "        target_value = torch.stack([x[target] for x in outputs])\n", "    else:\n", "        target_value = torch.cat([x[target] for x in outputs])\n", "    return target_value"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from commons import Attention, clones\n", "from commons import TransformerEncoder"]}, {"cell_type": "markdown", "metadata": {}, "source": [" ---------------- DECODER ----------------------- ##"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class TransformerDecoderLayer(nn.Module):\n", "    # - a single layer for Transformer-Decoder block\n", "    def __init__(self, d_model, num_head, droput, dim_feedforward, eps=1e-12):\n", "        super(TransformerDecoderLayer, self).__init__()\n", "        self.embed_dim = d_model \n", "        self.dropout = droput\n\n", "        ## self-attention\n", "        self.self_attn = Attention(self.embed_dim, num_head, droput)\n", "        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim) ## residual + LN\n\n", "        ## cross-attention over encoder's output\n", "        self.encoder_attn = Attention(self.embed_dim, num_head, droput)\n", "        self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim, eps=eps)\n\n", "        ## MLP\n", "        self.act_fc = nn.GELU()\n", "        self.activation_dropout = droput # same as hidden state dropout\n", "        \n", "        self.fc1 = nn.Linear(self.embed_dim, dim_feedforward)\n", "        self.fc2 = nn.Linear(dim_feedforward, self.embed_dim)\n", "        self.final_layer_norm = nn.LayerNorm(self.embed_dim, eps=eps)\n", "      \n", "    def forward(self, x, enc_output, look_ahead_mask, enc_pad_mask):\n", "        \"Follow Figure 1 (right) of the original paper for connections.\"\n", "        # enc_output : [B, input_seq_len, d_model]\n", "        # x : input \n", "        # look_ahead_mask : for decoder's input\n", "        # enc pad_mask    : for encoder's output\n\n", "        # 1) self-multihead-attention with add & norm \n", "        residual = x\n", "        x, dec_attn_scores = self.self_attn(query=x, key=x, value=x, mask=look_ahead_mask)\n", "        x = F.dropout(x, self.dropout, training=self.training)\n", "        x = residual + x \n", "        x = self.self_attn_layer_norm(x)\n\n", "        # 2) cross attention \n", "        residual = x\n", "        x, cross_attn_scores = self.encoder_attn(query=x, key=enc_output, value=enc_output, mask=enc_pad_mask)\n", "        x = F.dropout(x, self.dropout, training=self.training)\n", "        x = residual + x \n", "        x = self.encoder_attn_layer_norm(x)\n\n", "        # 3) MLP\n", "        residual = x\n", "        x = self.act_fc(self.fc1(x))\n", "        x = F.dropout(x, self.activation_dropout, training=self.training)\n", "        x = self.fc2(x)\n", "        x = F.dropout(x, self.dropout, training=self.training)\n", "        x = residual + x \n", "        x = self.final_layer_norm(x)\n", "        \n", "        # out : [batch_size, target_seq_len, d_model]\n", "        # attn_scores_1 : [batch_size, num_head, target_seq_len, target_seq_len] = [B, H, query_len, query_len]\n", "        # attn_scores_2 : [batch_size, num_head, target_seq_len, source_seq_len] = [B, H, key_len, query_len]\n", "        return x, dec_attn_scores, cross_attn_scores"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class TransformerDecoder(nn.Module):\n", "    \"Decoder Block - a stack of N layers\"\n", "    def __init__(self, num_layers, d_model, num_heads, dropout, dim_feedforward=None):\n", "        super(TransformerDecoder, self).__init__()\n", "        self.num_layers = num_layers\n", "        if dim_feedforward == None: dim_feedforward = 4*d_model\n", "        a_layer = TransformerDecoderLayer(d_model, num_heads, dropout, dim_feedforward)\n\n", "        # prepare N sub-blocks\n", "        self.layers = clones(a_layer, self.num_layers)\n", "        \n", "    def forward(self, x, enc_output, look_ahead_mask=None, enc_pad_mask=None):\n", "        # x : [B, tar_seq_len, d_model] \n", "        # enc_output : [B, src_seq_len, d_model] \n", "        # look_ahead_mask : for decoding (causual mask)\n", "        # enc_pad_mask : for blending encoder's hidden states(key) with decoder's input(query), \n", "        #                need to ignore 'pad' positioned hidden states.\n", "        layers_attn_scores_1 = []\n", "        layers_attn_scores_2 = []\n", "        \"Pass the input (and mask) through each layer in turn.\"\n", "        for layer in self.layers:\n", "            x, attn_scores_1, attn_scores_2 = layer(x, enc_output, look_ahead_mask, enc_pad_mask)\n", "            layers_attn_scores_1.append(attn_scores_1) # for encoder\n", "            layers_attn_scores_2.append(attn_scores_2) # for decoder\n", "        return x, layers_attn_scores_1, layers_attn_scores_2\n", "    \n", "## -------------------- TRANSFORMER (Encoder + Decoder) ----------------------- ##\n", "##  Additionally we need to implement\n", "##      - Embedding modules ( we will re-use BertEmbeddings ) with differnt name\n", "##          - wihtout token type embedding\n", "class InputEmbeddings(nn.Module):\n", "    \"\"\" this embedding moudles are from huggingface implementation\n", "        but, it is simplified -- removing token type embedding since it is not for BERT\n", "    \"\"\"\n", "    def __init__(self, vocab_size, hidden_size, pad_token_id, max_length_size, layer_norm_eps, hidden_dropout_prob):\n", "        super().__init__()\n", "        self.word_embeddings        = nn.Embedding(vocab_size, hidden_size, padding_idx=pad_token_id)\n", "        self.position_embeddings    = nn.Embedding(max_length_size, hidden_size)\n\n", "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n", "        # any TensorFlow checkpoint file\n", "        self.LayerNorm = nn.LayerNorm(hidden_size, eps=layer_norm_eps)\n", "        self.dropout   = nn.Dropout(hidden_dropout_prob)\n\n", "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n", "        self.register_buffer(\"position_ids\", torch.arange(max_length_size).expand((1, -1)))\n", "        self.register_buffer(\n", "            \"token_type_ids\",\n", "            torch.zeros(self.position_ids.size(), dtype=torch.long, device=self.position_ids.device),\n", "            persistent=False,\n", "        )\n\n", "        # always absolute\n", "        self.position_embedding_type = \"absolute\"\n", "    def forward(self, input_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0):\n", "        if input_ids is not None:\n", "            input_shape = input_ids.size()\n", "        else:\n", "            input_shape = inputs_embeds.size()[:-1]\n", "        seq_length = input_shape[1]\n", "        if position_ids is None:\n", "            position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n", "        if inputs_embeds is None:\n", "            inputs_embeds = self.word_embeddings(input_ids)\n", "        embeddings = inputs_embeds \n", "        if self.position_embedding_type == \"absolute\":\n", "            position_embeddings = self.position_embeddings(position_ids)\n", "            embeddings += position_embeddings\n", "        embeddings = self.LayerNorm(embeddings)\n", "        embeddings = self.dropout(embeddings)\n", "        return embeddings"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class Transformer(nn.Module):\n", "    # in here, embedding and decoder output processing parts are not included\n", "    def __init__(self, num_layers, d_model, num_heads, dropout, dim_feedforward=None):\n", "        super().__init__()\n\n", "        ## transformer blocks only \n", "        self.encoder = TransformerEncoder(num_layers, d_model, num_heads, dropout, dim_feedforward)\n", "        self.decoder = TransformerDecoder(num_layers, d_model, num_heads, dropout, dim_feedforward)\n", "    def create_padding_mask(self, mask):\n", "        # prepare padding mask for attention matrix compatible\n", "        return mask[:, None, None, :] # [B, 1, 1, seq_len] \n", "    def create_look_ahead_mask(self, seq_len):  \n", "        \"\"\"\n", "        prepare causual mask or look-ahead-mask for the decoding\n", "        In decoder, self-attention should be performed with only visible items\n", "        at each time steps. This mask is for preventing future-items at each self-attention in decoer \n", "        \"\"\"\n", "        mask = torch.triu(torch.ones(seq_len, seq_len, dtype=torch.int), diagonal=1)\n", "        mask = 1 - mask # reverse (1 for visible, 0 for invisible)\n", "        return mask\n", "    def forward(self, enc_input, dec_input, enc_pad_mask):\n", "        # enc_input : [B, src_len, d_model]\n", "        # dec_input : [B, tar_len, d_model]\n", "        #               - in training, it is a right-shifted decoder output starting with <start>\n", "        #               - in inference, it is a previous decoder output appended data starting with <start>\n", "        #\n", "        # enc_pad_mask : \n", "        #       - padding mask for encoder attention\n", "        #       - padding mask for decoder's 2nd attention (to blend encoder's outputs)\n\n", "        # --------\n", "        # encoder\n", "        # --------\n", "        enc_pad_mask = self.create_padding_mask(enc_pad_mask)\n", "        enc_output, enc_layer_att_scores = self.encoder(enc_input, enc_pad_mask)\n\n", "        # --------\n", "        # decoder\n", "        # --------\n", "        # masking for self-attention in decoder (LOOK-AHEAD)\n", "        dec_length = dec_input.size()[1] \n", "        look_ahead_mask = self.create_look_ahead_mask(dec_length).to(dec_input.device)\n\n", "        # masking for cross-attention in bleding decoder input(query) with encoder output(key, value)\n", "        # since multiple-items are from encoder, \n", "        # the mask should be encoder padding mask\n", "        dec_output, dec_layer_att_scores, dec_layer_cross_att_scores = self.decoder(\n", "                                                                                    dec_input, \n", "                                                                                    enc_output, \n", "                                                                                    look_ahead_mask=look_ahead_mask, \n", "                                                                                    enc_pad_mask=enc_pad_mask\n", "                                                                                )\n", "        return enc_output, dec_output, enc_layer_att_scores, dec_layer_att_scores, dec_layer_cross_att_scores"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from torch.utils.data import random_split\n", "from torchvision import transforms"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pytorch_lightning as pl"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from torch.utils.data import Dataset, DataLoader\n", "import numpy as np "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def load_data(fn):\n", "    data = []\n", "    with open(fn, 'r', encoding='utf-8') as f:\n", "        for line in f:\n", "            line = line.rstrip()\n", "            seq_str, sorted_seq_str = line.split('\\t')\n", "            seqs = seq_str.split(',')\n", "            sorted_seqs = sorted_seq_str.split(',')\n", "            data.append( (seqs, sorted_seqs) )\n", "    return data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class NumberDataset(Dataset):\n", "    \"\"\"Dataset.\"\"\"\n", "    def __init__(self, fn, input_vocab, output_vocab, max_enc_seq_length, max_dec_seq_length):\n", "        self.input_vocab = input_vocab\n", "        self.output_vocab = output_vocab\n", "        self.max_enc_seq_length = max_enc_seq_length \n", "        self.max_dec_seq_length = max_dec_seq_length \n", "        \n", "        # load \n", "        self.data = load_data(fn)\n", "    def __len__(self):\n", "        return len(self.data) \n", "    def __getitem__(self, idx): \n", "        seq, sorted_seq = self.data[idx]\n", "        sorted_seq_ids = [ self.output_vocab[t] for t in sorted_seq ]\n\n", "        # [ input ] ---------------------------------------------\n", "        # \n", "        # < encoder input > \n", "        seq_ids = [ self.input_vocab[t] for t in seq ]\n", "        # <pad> processing\n", "        pad_id      = self.input_vocab['<pad>']\n", "        num_to_fill = self.max_enc_seq_length - len(seq)\n", "        seq_ids     = seq_ids + [pad_id]*num_to_fill\n\n", "        # mask processing (1 for valid, 0 for invalid)\n", "        enc_pad_mask = [1]*len(seq) + [0]*num_to_fill\n\n", "        # < decoder input> \n", "        #   - <start> should be added at first position\n", "        dec_input_seq_ids = [ self.output_vocab['<start>']] + sorted_seq_ids \n\n", "        # <pad> processing\n", "        pad_id = self.output_vocab['<pad>']\n", "        num_to_fill = self.max_dec_seq_length - len(dec_input_seq_ids)\n", "        dec_input_seq_ids = dec_input_seq_ids + [pad_id]*num_to_fill\n\n", "        # [ output ] ---------------------------------------------\n", "        #   - <end> should be  added at the last position\n", "        pad_id = self.output_vocab['<pad>']\n", "        dec_output_seq_ids = sorted_seq_ids + [ self.output_vocab['<end>']]\n", "        \n", "        # mask processing (1 for valid, 0 for invalid)\n", "        # this mask can be used penalize unnecessary loss value at calcuation cross-entropy \n", "        dec_output_pad_mask = [1]*len(dec_output_seq_ids) + [0]*num_to_fill\n", "        num_to_fill = self.max_dec_seq_length - len(dec_output_seq_ids)\n", "        dec_output_seq_ids = dec_output_seq_ids + [pad_id]*num_to_fill\n", "        item = [\n", "                    # input - encoder \n", "                    np.array(seq_ids),\n", "                    np.array(enc_pad_mask),  # encoder padding\n", "                    # input - decoder\n", "                    np.array(dec_input_seq_ids),\n", "                    # output - decoder\n", "                    np.array(dec_output_seq_ids),\n", "                    np.array(dec_output_pad_mask),  # decoder output padding\n", "               ]\n", "        return item "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class NumberDataModule(pl.LightningDataModule):\n", "    def __init__(self, \n", "                 max_enc_seq_length: int=20,\n", "                 max_dec_seq_length: int=13, # for testing\n", "                 batch_size: int = 32):\n", "        super().__init__()\n", "        self.batch_size = batch_size\n", "        self.max_enc_seq_length = max_enc_seq_length \n", "        self.max_dec_seq_length = max_dec_seq_length \n", "        input_vocab, output_vocab = self.make_vocab('./data/sorted_numbers/train.txt')\n", "        self.input_vocab_size = len( input_vocab )\n", "        self.output_vocab_size = len( output_vocab )\n", "        self.input_vocab = input_vocab\n", "        self.output_vocab = output_vocab\n", "        self.enc_padding_idx = input_vocab['<pad>']\n", "        self.dec_padding_idx = output_vocab['<pad>']\n", "        self.all_train_dataset = NumberDataset('./data/sorted_numbers/train.txt', input_vocab, output_vocab, max_enc_seq_length, max_dec_seq_length)\n", "        self.test_dataset      = NumberDataset('./data/sorted_numbers/test.txt', input_vocab, output_vocab, max_enc_seq_length, max_dec_seq_length)\n", "        self.max_enc_seq_length = max_enc_seq_length\n", "        self.max_dec_seq_length = max_dec_seq_length "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["        # random split train / valiid for early stopping\n", "        N = len(self.all_train_dataset)\n", "        tr = int(N*0.8) # 8 for the training\n", "        va = N - tr     # 2 for the validation \n", "        self.train_dataset, self.valid_dataset = torch.utils.data.random_split(self.all_train_dataset, [tr, va])\n", "    def make_vocab(self, fn):\n", "        input_tokens = []\n", "        output_tokens = []\n", "        data = load_data(fn)\n", "        for seqs, sorted_seqs in data:\n", "            for token in seqs:\n", "                input_tokens.append(token)\n", "            for token in sorted_seqs:\n", "                output_tokens.append(token)\n", "        \n", "        input_tokens = list(set(input_tokens))\n", "        output_tokens = list(set(output_tokens)) \n", "        input_tokens.sort()\n", "        output_tokens.sort()\n\n", "        # [encoder vocab]\n", "        #   - add <pad> symbol to input tokens as a first item\n", "        input_tokens = ['<pad>'] + input_tokens \n", "        input_vocab = { str(token):index for index, token in enumerate(input_tokens) }\n\n", "        # [decoder vocab]\n", "        output_tokens = ['<pad>', '<start>', '<end>'] + output_tokens  ## note that we need <start> and <end> for decoding\n", "        output_vocab = { str(token):index for index, token in enumerate(output_tokens) }\n", "        return input_vocab, output_vocab\n", "    def train_dataloader(self):\n", "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True) # NOTE : Shuffle\n", "    def val_dataloader(self):\n", "        return DataLoader(self.valid_dataset, batch_size=self.batch_size)\n", "    def test_dataloader(self):\n", "        return DataLoader(self.test_dataset, batch_size=self.batch_size)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from torchmetrics import functional as FM"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class Transformer_Number_Sorting(pl.LightningModule): \n", "    def __init__(self, \n", "                 input_vocab_size,\n", "                 output_vocab_size,\n", "                 num_layers,   # number of layers\n", "                 d_model,      # dim. in attemtion mechanism \n", "                 num_heads, \n", "                 enc_padding_idx,\n", "                 dec_padding_idx,\n", "                 enc_max_length,\n", "                 dec_max_length,\n", "                 # optiimzer setting\n", "                 learning_rate=1e-3):\n", "        super().__init__()\n", "        self.save_hyperparameters()  \n\n", "        ## embeddings for encoder and decoder (not shared so far)\n", "        self.enc_emb = InputEmbeddings(\n", "                                           vocab_size=self.hparams.input_vocab_size,\n", "                                           hidden_size=self.hparams.d_model,\n", "                                           pad_token_id=self.hparams.enc_padding_idx,\n", "                                           max_length_size=self.hparams.enc_max_length,\n", "                                           layer_norm_eps=1e-12,\n", "                                           hidden_dropout_prob=0.1\n", "                                       )\n", "        self.dec_emb = InputEmbeddings(\n", "                                           vocab_size=self.hparams.output_vocab_size,\n", "                                           hidden_size=self.hparams.d_model,\n", "                                           pad_token_id=self.hparams.dec_padding_idx,\n", "                                           max_length_size=self.hparams.dec_max_length,\n", "                                           layer_norm_eps=1e-12,\n", "                                           hidden_dropout_prob=0.1\n", "                                       )\n\n", "        ## Transformer Block\n", "        self.transformer = Transformer(\n", "                                        num_layers=self.hparams.num_layers, \n", "                                        d_model=self.hparams.d_model, \n", "                                        num_heads=self.hparams.num_heads,\n", "                                        dropout=0.1,\n", "                                        dim_feedforward= 4*self.hparams.d_model\n", "                                       )\n\n", "        ## to output class\n", "        self.to_output = nn.Linear(self.hparams.d_model, self.hparams.output_vocab_size) # D -> a single number\n\n", "        # loss\n", "        self.criterion = nn.CrossEntropyLoss(ignore_index=dec_padding_idx)\n", "        \n", "    def get_class_weights(self, end_idx):\n", "        ## weighting for <end> symbol output\n", "        end_idx = 2 \n", "        N = self.hparams.output_vocab_size \n", "        weight_for_end = 0.3\n", "        weight_for_others = (1.0 - weight_for_end) / (N-1) # except end\n", "        cls_weights = torch.ones(N) * weight_for_others\n", "        cls_weights[end_idx] = weight_for_end\n", "        return cls_weights\n", "    def forward(self, enc_input_id, dec_input_id, enc_input_pad_mask):\n", "        # ----------------------- ENCODING -------------------------------#\n", "        # [ Digit Character Embedding ]\n", "        #   for encoder and decoder\n", "        enc_input = self.enc_emb(enc_input_id.long())\n", "        dec_input = self.dec_emb(dec_input_id.long())\n", "        \n", "        enc_output, dec_output, layer_enc_attn_scores, layer_dec_attn_scores_1, layer_dec_attn_scores_2  =\\\n", "           self.transformer(enc_input, dec_input, enc_input_pad_mask)\n\n", "        # to symbol \n", "        step_logits = self.to_output(dec_output) # [B, tar_seq_len, num_output_vocab]\n", "        additional_outputs = [layer_enc_attn_scores, layer_dec_attn_scores_1, layer_dec_attn_scores_2]\n", "        return step_logits, additional_outputs\n", "    def training_step(self, batch, batch_idx):\n", "        enc_input_ids,  enc_input_pad_mask, \\\n", "        dec_input_ids,  \\\n", "        dec_output_ids, dec_output_pad_mask = batch \n", "        step_logits, _ = self(enc_input_ids, dec_input_ids, enc_input_pad_mask)\n", "        C = step_logits.size()[-1]\n", "        loss = self.criterion(step_logits.view(-1, C), dec_output_ids.view(-1).long()) \n", "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n", "        return loss\n", "    def validation_step(self, batch, batch_idx):\n", "        enc_input_ids,  enc_input_pad_mask, \\\n", "        dec_input_ids,  \\\n", "        dec_output_ids, dec_output_pad_mask = batch \n", "        step_logits, _ = self(enc_input_ids, dec_input_ids, enc_input_pad_mask)\n", "        C = step_logits.size()[-1]\n", "        loss = self.criterion(step_logits.view(-1, C), dec_output_ids.view(-1).long()) \n", "        \n", "        ## get predicted result\n", "        metrics = {'val_loss': loss}\n", "        self.log_dict(metrics)\n", "        return metrics\n", "    def validation_step_end(self, val_step_outputs):\n", "        val_loss = val_step_outputs['val_loss'].cpu()\n", "        self.log('validation_loss', val_loss, prog_bar=True)\n", "    def test_step(self, batch, batch_idx):\n", "        enc_input_ids,  enc_input_pad_mask, \\\n", "        dec_input_ids,  \\\n", "        dec_output_ids, dec_output_pad_mask = batch \n", "        step_logits, _ = self(enc_input_ids, dec_input_ids, enc_input_pad_mask)\n", "        C = step_logits.size()[-1]\n", "        loss = self.criterion(step_logits.view(-1, C), dec_output_ids.view(-1).long()) \n", "        \n", "        step_probs    = torch.softmax(step_logits, axis=-1) # [B, tar_seq_len, num_output_vocab]\n", "        step_best_ids = torch.argmax(step_probs, axis=-1)\n\n", "        ## prediction \n", "        result = {}\n", "        result['input'] = enc_input_ids.cpu()\n", "        result['predicted'] = step_best_ids.cpu()\n", "        result['reference'] = dec_output_ids.cpu()\n", "        result['step_probs'] = step_probs.cpu()\n", "        return result\n", "    def set_vocabs(self, input_vocab, output_vocab):\n", "        self.input_vocab = input_vocab\n", "        self.output_vocab = output_vocab\n", "        self.r_input_vocab = {v:k for k, v in input_vocab.items() }\n", "        self.r_output_vocab = {v:k for k, v in output_vocab.items() }"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    def test_epoch_end(self, outputs):\n", "        input      = result_collapse(outputs, 'input').cpu()\n", "        predicted  = result_collapse(outputs, 'predicted').cpu()\n", "        reference  = result_collapse(outputs, 'reference').cpu()\n", "        step_probs = result_collapse(outputs, 'step_probs').cpu()\n", "        def get_valid_items(tensor, pad_idx):\n", "            a = tensor.data.cpu().numpy()\n", "            a = a[ a != pad_idx ]\n", "            return a \n", "        import os\n", "        os.makedirs(\"./outputs\", exist_ok=True)\n", "        with open('./outputs/sorted_result.txt', 'w') as f:\n", "            for _input, _pred, _ref, _prob in zip(input, predicted, reference, step_probs):\n", "                _input = get_valid_items(_input, self.hparams.enc_padding_idx)\n", "                _pred = get_valid_items(_pred, self.hparams.dec_padding_idx)\n", "                _ref = get_valid_items(_ref, self.hparams.dec_padding_idx)\n", "                ## trim _pred with first <end>\n", "                _N = -1\n", "                for idx, _i in enumerate(_pred):\n", "                    if _i == self.output_vocab['<end>']: \n", "                        _N = idx\n", "                        break\n", "                _pred = _pred[:_N]\n", "                input_seq = [ self.r_input_vocab[x] for x in _input ]\n", "                pred_seq =  [ self.r_output_vocab[x] for x in _pred ]\n", "                ref_seq =   [ self.r_output_vocab[x] for x in _ref if self.output_vocab['<end>'] != x ]\n", "                \n", "                input_seq = \",\".join(input_seq)\n", "                pred_seq = \",\".join(pred_seq)\n", "                ref_seq = \",\".join(ref_seq)\n", "                flag = 'O' if pred_seq == ref_seq else 'X'\n", "                print(f'[{flag}] {input_seq}', file=f)\n", "                print(f'\\t\\tREF  : {ref_seq}', file=f)\n", "                print(f'\\t\\tPRED : {pred_seq}', file=f)\n", "                print(f'-------------------', file=f)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    def configure_optimizers(self):\n", "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n", "        return optimizer\n", "    @staticmethod\n", "    def add_model_specific_args(parent_parser):\n", "        parser = parent_parser.add_argument_group(\"Transformer\")\n", "        parser.add_argument('--learning_rate', type=float, default=0.0001)\n", "        return parent_parser"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from argparse import ArgumentParser\n", "from pytorch_lightning.callbacks import EarlyStopping\n", "def cli_main():\n", "    pl.seed_everything(1234)\n\n", "    # ------------\n", "    # args\n", "    # ------------\n", "    ## Transformer is very sensitive to sorting task  \n", "    ## Good settings so far \n", "    parser = ArgumentParser()\n", "    parser.add_argument('--batch_size', default=200, type=int)\n", "    parser.add_argument('--d_model',    default=512, type=int)  # dim. for attention model -- 'H' at paper\n", "    parser.add_argument('--num_heads',  default=8,   type=int)  # number of heads  -- 'A' at paper\n", "    parser.add_argument('--num_layers', default=8,   type=int)  # number of layers -- 'L' at paper\n", "    parser = pl.Trainer.add_argparse_args(parser)\n", "    parser = Transformer_Number_Sorting.add_model_specific_args(parser)\n", "    args = parser.parse_args()\n\n", "    # ------------\n", "    # data\n", "    # ------------\n", "    dm = NumberDataModule.from_argparse_args(args)\n", "    x = iter(dm.train_dataloader()).next() # <for testing \n\n", "    ## ------------\n", "    ## model\n", "    ## ------------\n", "    model = Transformer_Number_Sorting(\n", "                                    dm.input_vocab_size,\n", "                                    dm.output_vocab_size,\n", "                                    args.num_layers,    # number of layers \n", "                                    args.d_model,       # dim. in attemtion mechanism \n", "                                    args.num_heads,     # number of heads\n", "                                    dm.enc_padding_idx,\n", "                                    dm.dec_padding_idx,\n", "                                    dm.max_enc_seq_length,\n", "                                    dm.max_dec_seq_length,\n", "                                    args.learning_rate\n", "                                    )\n\n", "    # ------------\n", "    # training\n", "    # ------------\n", "    trainer = pl.Trainer(\n", "                            max_epochs=1, \n", "                            callbacks=[EarlyStopping(monitor='val_loss')],\n", "                            gpus = 1 # if you have gpu -- set number, otherwise zero\n", "                        )\n", "    trainer.fit(model, datamodule=dm)\n\n", "    # copy \n", "    import shutil\n", "    best_model_fn = trainer.checkpoint_callback.best_model_path\n", "    import os; os.makedirs('./outputs/release/', exist_ok=True)\n", "    shutil.copy(best_model_fn, './outputs/release/release.ckpt')\n\n", "    # ------------\n", "    # testing\n", "    # ------------\n", "    transformer_model = model.load_from_checkpoint('./outputs/release/release.ckpt')\n", "    model.set_vocabs(dm.input_vocab, dm.output_vocab)\n", "    result = trainer.test(model, test_dataloaders=dm.test_dataloader())"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == '__main__':\n", "    cli_main()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}