{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "    Attention Related codes and modules<br>\n", "    <br>\n", "    Author : Sangkeun Jung (hugmanskj@gmai.com)<br>\n", "    All rights reserved. (2021)<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["In this code, we will implement<br>\n", "  - Scaled Dot-Product attention mechanism <br>\n", "  - Query Key Value attention <br>\n", "  - Multihead attention"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import math \n", "def scaled_dot_product_attention(   q: torch.Tensor, \n", "                                    k: torch.Tensor, \n", "                                    v: torch.Tensor,                                  \n", "                                    mask: torch.Tensor = None,\n", "                                    dropout: float = 0.1,\n", "                                 ) -> torch.Tensor:\n", "    \"\"\"\n", "        In here, we try to calculate all multi-heads attentions at once. \n", "        So, we assumed that the first dimension of q, k and v is B*num_heads=...\n", "            q : expect [..., query_seq_len, d_k]\n", "            k : expect [..., key_seq_len,   d_k]\n", "            v : expect [..., key_seq_len,   d_v]\n", "        mask : expect extended shaped [B, num_heads, query_seq_len, key_seq_len] 1.0 for attend elements, 0 for masking elements\n", "        dropout : expect float value. \n", "    \"\"\"\n", "    # for scaling\n", "    d_k = k.size()[-1]\n", "    attn = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k) # [B, num_heads, query_seq_len, key_seq_len] \n\n", "    # masking \n", "    if mask != None:\n", "        inverted_mask = 1.0 - mask\n", "        inverted_mask = inverted_mask.masked_fill(inverted_mask.bool(), torch.finfo(attn.dtype).min)\n", "        attn = attn + inverted_mask  # checkout before and after attn[0][0][0], mask[0][0][0]\n\n", "    # calculate softmax \n", "    attention_weights = F.softmax(attn, dim=-1)  # over key dimension   # [..., seq_len, d_k]\n\n", "    # Original Paper didn't mention about dropout on attention weights. \n", "    # But many working architectures use dropout on attentions \n", "    # so, in here we will apply dropout on scores\n", "    if type(dropout) == float : \n", "        attention_weights = F.dropout(attention_weights, dropout)\n", "    else: \n", "        attention_weights = dropout(attention_weights)\n\n", "    # blending\n", "    output = torch.matmul(attention_weights, v)\n", "    return output, attention_weights"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class Attention(nn.Module):\n", "    ## this Attention implementation is almost identical to original transformer paper.\n", "    def __init__(self, d_model, num_heads, dropout=0.1, use_bias=True):\n", "        super(Attention, self).__init__()\n", "        assert d_model % num_heads == 0\n", "        self.num_heads = num_heads\n\n", "        # We assume d_v always equals d_k\n", "        self.d_k = d_model // num_heads  # ex) d_model = 512, num_head = 8 --> d_k = 64\n", "        self.d_v = d_model // num_heads  # ex) d_model = 512, num_head = 8 --> d_v = 64\n\n", "        # why * num_head? --> preapre N heads's input\n", "        # d_model = self.d_k * self.num_head\n", "        # \n", "        # there are variations to use 'biases' in q,k,v, and o \n", "        # but, in this implementation, we will use bias \n", "        self.wq = nn.Linear(d_model, d_model, bias=use_bias) \n", "        self.wk = nn.Linear(d_model, d_model, bias=use_bias) \n", "        self.wv = nn.Linear(d_model, d_model, bias=use_bias) \n\n", "        # dropout\n", "        self.dropout = nn.Dropout(dropout)\n\n", "        # to make output \n", "        # we follow original transformer paper : \n", "        # in the paper, they mentioned WO for projection on concat vector.\n", "        self.wo = nn.Linear(d_model, d_model, bias=use_bias)\n", "    def split_heads(self, x, batch_size):\n", "        # split the projected dimension \n", "        # [B, seq_len, heads * d_k ] --> [B, heads, seq_len, d_k] \n", "        x = x.view(batch_size, -1, self.num_heads, self.d_k) # to be [B, seq_len, heads, d_k]\n", "        x = x.transpose(1,2).contiguous()  # to be [B, heads, seq_len, d_k]\n", "        return x\n", "    def forward(self, query, key, value, mask=None):\n", "        q = self.wq(query)      # d_k --> d_k*num_head\n", "        k = self.wk(key)        # d_k --> d_k*num_head\n", "        v = self.wv(value)      # d_k --> d_k*num_head\n", "        \n", "        # shape change to [B, heads, seq_len, d_k]\n", "        _, qS = q.size()[0], q.size()[1] # qS = query_seq_len \n", "        B, S  = k.size()[0], k.size()[1] # S  = key_seq_len\n", "        \n", "        q = self.split_heads(q, B) # [B, num_heads, query_seq_len, d_k]\n", "        k = self.split_heads(k, B) # [B, num_heads, key_seq_len,   d_k]\n", "        v = self.split_heads(v, B) # [B, num_heads, key_seq_len,   d_k]\n\n", "        # scaled dot-product attention\n", "        # scaled_attention  = [..., query_seq_len, d_k]\n", "        # attention_weights = [..., query_seq_len, key_seq_len]\n", "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask, self.dropout)\n", "        \n", "        # [Concat Process - for merging multiheads] \n", "        # recover the tensor form\n", "        scaled_attention = scaled_attention.transpose(1,2)     # to be [B, query_seq_len, num_heads, d_k]\n", "        \n", "        # concat\n", "        concat_attention = scaled_attention.reshape(B, qS, -1) # to be [B, query_seq_len, (num_heads*d_k)=d_model]\n\n", "        # to output\n", "        output = self.wo(concat_attention) \n\n", "        # output : # [B, query_seq_len, d_model]\n", "        # attention_weights : [B, num_heads, query_seq_len, key_seq_len]\n", "        return output, attention_weights "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class TransformerEncoderLayer(nn.Module):\n", "    # - a single layer for Transformer-Encoder block\n", "    # - This Encoder block is almost identical to original transformer block\n", "    # - activation function is changed to RELU \n", "    #       - (note that, recently RELU is frequently replaced as GELU)\n", "    def __init__(self, d_model, num_head, dim_feedforward, dropout=0.1):\n", "        super(TransformerEncoderLayer, self).__init__()\n", "        self.dropout = dropout\n\n", "        # self-attention\n", "        self.self_attn = Attention(d_model, num_head, dropout)\n\n", "        # MLP\n", "        self.act_fc = nn.GELU() # <- I changed RELU to GELU \n", "        self.fc1 = nn.Linear(d_model, dim_feedforward)\n", "        self.fc2 = nn.Linear(dim_feedforward, d_model)\n\n", "        # LN for after attention and final \n", "        self.self_attn_layer_norm = nn.LayerNorm(d_model)\n", "        self.final_layer_norm     = nn.LayerNorm(d_model)\n", "    def forward(self, x, mask):\n", "        # 1) self-multihead-attention with add & norm \n", "        residual = x\n", "        x, attn_scores = self.self_attn(query=x, key=x, value=x, mask=mask)\n", "        x = F.dropout(x, self.dropout, training=self.training)\n", "        x = residual + x \n", "        x = self.self_attn_layer_norm(x) # POST Layer Normalization\n\n", "        # 2) MLP with add & norm\n", "        residual = x\n", "        x = self.act_fc(self.fc1(x))\n", "        x = F.dropout(x, self.dropout, training=self.training)\n", "        x = self.fc2(x)\n", "        x = F.dropout(x, self.dropout, training=self.training)\n", "        x = residual + x \n", "        x = self.final_layer_norm(x)     # POST Layer Normalization\n\n", "        # out : [batch_size, step_size=S, d_model]\n", "        return x, attn_scores"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["   \n", "import copy \n", "def clones(module, N):\n", "    \"Produce N identical layers.\"\n", "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class TransformerEncoder(nn.Module):\n", "    # Encoder Block - a stack of N layers\n", "    # Exactly same as TransformerEncoder \n", "    def __init__(self, num_layers, d_model, num_heads, dropout, dim_feedforward=None):\n", "        super(TransformerEncoder, self).__init__()\n", "        self.num_layers = num_layers\n", "        if dim_feedforward == None: dim_feedforward = 4*d_model  ## https://arxiv.org/pdf/1810.04805.pdf (page3)\n", "        \n", "        a_layer = TransformerEncoderLayer(d_model, num_heads, dim_feedforward, dropout)\n\n", "        # prepare N sub-blocks\n", "        self.layers = clones(a_layer, self.num_layers)\n", "        \n", "    def forward(self, x, mask=None):\n", "        # x expects : [B, seq_len, d_model] \n", "        layers_attn_scores = []\n", "        \"Pass the input (and mask) through each layer in turn.\"\n", "        for layer in self.layers:\n", "            x, attn_scores = layer(x, mask)\n", "            layers_attn_scores.append(attn_scores)\n", "        return x, layers_attn_scores"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def cp_weight(src, tar, copy_bias=True, include_eps=False):\n", "    assert tar.weight.size() == src.weight.size(), \"Not compatible parameter size\"\n", "    tar.load_state_dict( src.state_dict() )\n", "    \n", "    if include_eps:\n", "        # in case of LayerNorm. \n", "        with torch.no_grad():\n", "            tar.eps = src.eps  \n\n", "    ## call by reference\n", "    ## therefore, tar value is changed in this func. \n", "      "]}, {"cell_type": "markdown", "metadata": {}, "source": [" ----------------------- Applied Transformer Network ---------------------- ##"]}, {"cell_type": "markdown", "metadata": {}, "source": ["-------------------------------------------------------------------------- #<br>\n", "BERT Implementation<br>\n", "-------------------------------------------------------------------------- #<br>\n", "BERT has three parts<br>\n", "  - Embedding Part ( we will re-use huggingface code)<br>\n", "      - symbol embedding<br>\n", "      - position embedding<br>\n", "      - type embedding<br>\n", "<br>\n", "  - Transformer Encoder Blocks (we will user our own code)<br>\n", "<br>\n", "  - Pooling Part (we will re-use huggingface code)<br>\n", "<br>\n", "  Note that<br>\n", "      - embedding and pooling part varies a lot according to transformer researches.<br>\n", "-------------------------------------------------------------------------- #"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Embedding and Pooling<br>\n", "this embedding is from huggingface"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class BertEmbeddings(nn.Module):\n", "    \"\"\" this embedding moudles are from huggingface implementation\n", "        but, it is simplified for just testing \n", "    \"\"\"\n", "    def __init__(self, vocab_size, hidden_size, pad_token_id, max_bert_length_size, layer_norm_eps, hidden_dropout_prob):\n", "        super().__init__()\n", "        self.word_embeddings        = nn.Embedding(vocab_size, hidden_size, padding_idx=pad_token_id)\n", "        self.position_embeddings    = nn.Embedding(max_bert_length_size, hidden_size)\n", "        self.token_type_embeddings  = nn.Embedding(2, hidden_size) # why 2 ? 0 and 1 \n\n", "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n", "        # any TensorFlow checkpoint file\n", "        self.LayerNorm = nn.LayerNorm(hidden_size, eps=layer_norm_eps)\n", "        self.dropout   = nn.Dropout(hidden_dropout_prob)\n\n", "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n", "        self.register_buffer(\"position_ids\", torch.arange(max_bert_length_size).expand((1, -1)))\n", "        self.register_buffer(\n", "            \"token_type_ids\",\n", "            torch.zeros(self.position_ids.size(), dtype=torch.long, device=self.position_ids.device),\n", "            persistent=False,\n", "        )\n\n", "        # always absolute\n", "        self.position_embedding_type = \"absolute\"\n", "    def forward(self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0):\n", "        if input_ids is not None:\n", "            input_shape = input_ids.size()\n", "        else:\n", "            input_shape = inputs_embeds.size()[:-1]\n", "        seq_length = input_shape[1]\n", "        if position_ids is None:\n", "            position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n\n", "        # Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs\n", "        # when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves\n", "        # issue #5664\n", "        if token_type_ids is None:\n", "            if hasattr(self, \"token_type_ids\"):\n", "                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n", "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n", "                token_type_ids = buffered_token_type_ids_expanded\n", "            else:\n", "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n", "        if inputs_embeds is None:\n", "            inputs_embeds = self.word_embeddings(input_ids)\n", "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n", "        embeddings = inputs_embeds + token_type_embeddings\n", "        if self.position_embedding_type == \"absolute\":\n", "            position_embeddings = self.position_embeddings(position_ids)\n", "            embeddings += position_embeddings\n", "        embeddings = self.LayerNorm(embeddings)\n", "        embeddings = self.dropout(embeddings)\n", "        return embeddings"]}, {"cell_type": "markdown", "metadata": {}, "source": ["this pooler is from huggingface"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class BertPooler(nn.Module):\n", "    def __init__(self, hidden_size):\n", "        super().__init__()\n", "        self.dense = nn.Linear(hidden_size, hidden_size)\n", "        self.activation = nn.Tanh()\n", "    def forward(self, hidden_states):\n", "        # We \"pool\" the model by simply taking the hidden state corresponding\n", "        # to the first token.\n", "        first_token_tensor = hidden_states[:, 0]\n", "        pooled_output = self.dense(first_token_tensor)\n", "        pooled_output = self.activation(pooled_output)\n", "        return pooled_output"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class BERT_CONFIG():\n", "    def __init__(self, vocab_size, padding_idx, max_seq_length, \n", "                       d_model, layer_norm_eps, emb_hidden_dropout,\n", "                       num_layers, num_heads, att_prob_dropout, dim_feedforward\n", "                 ):\n", "        # embedding\n", "        self.vocab_size = vocab_size\n", "        self.padding_idx = padding_idx\n", "        self.max_seq_length = max_seq_length\n", "        self.d_model = d_model\n", "        self.layer_norm_eps = layer_norm_eps\n", "        self.emb_hidden_dropout = emb_hidden_dropout\n\n", "        # attention\n", "        self.num_layers=num_layers\n", "        self.num_heads = num_heads\n", "        self.att_prob_dropout = att_prob_dropout\n", "        self.dim_feedforward = dim_feedforward"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [" \n", "                             "]}, {"cell_type": "markdown", "metadata": {}, "source": [" We will wrap-all BERT sub-processing as BERT module"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class BERT(nn.Module):\n", "    def __init__(self, config):\n", "        super().__init__()\n\n", "        ## [Embeddings] \n", "        self.embeddings = BertEmbeddings(\n", "                                            config.vocab_size ,\n", "                                            config.d_model,\n", "                                            config.padding_idx,\n", "                                            config.max_seq_length,\n", "                                            config.layer_norm_eps,    # layer norm eps\n", "                                            config.emb_hidden_dropout # 0.1    \n", "                                    )\n", "        ## [Transformers]\n", "        self.encoder = TransformerEncoder(\n", "                                        num_layers=config.num_layers, \n", "                                        d_model=config.d_model, \n", "                                        num_heads=config.num_heads,\n", "                                        dropout=config.att_prob_dropout,\n", "                                        dim_feedforward=config.dim_feedforward\n", "                                )"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["        ## [Pooler]\n", "        self.pooler = BertPooler(config.d_model)\n", "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n", "        attention_mask = attention_mask[:, None, None, :] # [B, 1, 1, seq_len] \n", "        seq_embs   = self.embeddings(input_ids, token_type_ids)\n", "        output     = self.encoder(seq_embs, attention_mask)\n", "        pooled_out = self.pooler(output[0]) \n", "        layer_attention_scores = output[1]\n", "        return pooled_out, layer_attention_scores\n", "    def cp_encoder_block_weights_from_huggingface(self, src_encoder, tar_encoder):\n", "        ## src: huggingface BERT model\n", "        ## tar: my BERT model \n", "        for layer_num, src_layer in enumerate(src_encoder.layer):\n", "            # <<< to MultiHeadAttention (wq, wk, wv, wo) >>>\n", "            cp_weight(src_layer.attention.self.query,   tar_encoder.layers[layer_num].self_attn.wq) # wq\n", "            cp_weight(src_layer.attention.self.key,     tar_encoder.layers[layer_num].self_attn.wk) # wk\n", "            cp_weight(src_layer.attention.self.value,   tar_encoder.layers[layer_num].self_attn.wv) # wv\n", "            cp_weight(src_layer.attention.output.dense, tar_encoder.layers[layer_num].self_attn.wo) # wo\n\n", "            # <<< to MLP (fc1, fc2) >>>\n", "            cp_weight(src_layer.intermediate.dense, tar_encoder.layers[layer_num].fc1) # feed_forward_1\n", "            cp_weight(src_layer.output.dense,       tar_encoder.layers[layer_num].fc2) # feed_forward_2\n\n", "            # layer normalization parameters\n", "            cp_weight(src_layer.attention.output.LayerNorm, tar_encoder.layers[layer_num].self_attn_layer_norm, include_eps=True) # norm_1\n", "            cp_weight(src_layer.output.LayerNorm,           tar_encoder.layers[layer_num].final_layer_norm, include_eps=True) # norm_2\n", "        return tar_encoder\n", "    def copy_weights_from_huggingface(self, hg_bert):\n", "        self.embeddings.load_state_dict( hg_bert.embeddings.state_dict() ) \n", "        self.pooler.load_state_dict( hg_bert.pooler.state_dict() ) \n", "        self.encoder = self.cp_encoder_block_weights_from_huggingface(\n", "                                            src_encoder=hg_bert.encoder,\n", "                                            tar_encoder=self.encoder \n", "                                          )"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}